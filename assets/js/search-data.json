{
  
    
        "post0": {
            "title": "Analyzing my Spotify listening history 🎵 - Part 3",
            "content": "In part 3 we are going to take a look at the audio features that Spotify labels songs with. They have 9 self-determined variables, that all indicate different aspects of a song. You&#39;ll learn all about them below. In part 2, we looked at how my genres are and changed over time. Now we&#39;ll take a look at how the audio features correspond to the changes in genres and how the genres are defined in the sense of these features. . The main research questions of part 3 are: . How are my audio features different from the general population? | How are these differences explained by my genres of preference? | Have some of these audio features changed over time, similar to my preference in genres has changed over time? | Lets start! . Loading previous data &#8987; . Let&#39;s load up everything and put the track IDs in a list, for easier access. Those are used for scraping. . #collapse-hide def get_track_info(): lines = [] for i in json_lines.reader(open(&#39;data/EndSong.json&#39;, encoding=&#39;utf-8&#39;)): lines.append(i) df = pd.DataFrame(lines) df = df.drop([&#39;username&#39;, &#39;user_agent_decrypted&#39;, &#39;incognito_mode&#39;, &#39;platform&#39;, &#39;ip_addr_decrypted&#39;, &#39;region&#39;, &#39;longitude&#39;, &#39;latitude&#39;, &#39;city&#39;], axis=1) df.ts = pd.to_datetime(df.ts) return df track_info = get_track_info() track_ids = pd.read_csv(&#39;track_ids.csv&#39;) track_ids = track_ids.values.reshape(-1).tolist() top_1_genres = pd.read_csv(&#39;top_1_genres.csv&#39;) top_1_genres_filtered = pd.read_csv(&#39;top_1_genres_filtered.csv&#39;) . . Audio Features &#127899;&#65039; . Using the Spotify API to get audio features . We need to retrieve the audio features for our songs. For this, we use the song IDs we retrieved in part 1. Lets first define our spotify API credentials again, an initialize the library we use to interact, Spotipy. . #collapse-hide SPOTIFY_API_KEY = os.getenv(&#39;SPOTIFY_API_KEY&#39;) SPOTIFY_CLIENT_ID = os.getenv(&#39;SPOTIFY_CLIENT_ID&#39;) spotify_search_url = &#39;https://api.spotify.com/v1/search?q={q}&amp;type={type}&#39; spotify_headers = {&quot;Accept&quot;: &quot;application/json&quot;, &quot;Authorization&quot;:&quot;Bearer &quot; + SPOTIFY_API_KEY, &quot;Content-Type&quot;: &quot;application/json&quot;} client_credentials_manager = SpotifyClientCredentials(SPOTIFY_CLIENT_ID, SPOTIFY_API_KEY) spotify = spotipy.Spotify(client_credentials_manager=client_credentials_manager) . . We can utilize the .audio_features call (docs) to retrieve a lot of information about a track. We give this call a Spotify track URI, so Spotify knows exactly which track the request is about. However, some songs did not give us an URI when we were getting those. So the resulting track IDs have some None in them. . Lets see what the audio features are of Blue from Gemini. . { &#39;danceability&#39;: 0.455, &#39;energy&#39;: 0.897, &#39;key&#39;: 9, &#39;loudness&#39;: -4.879, &#39;mode&#39;: 1, &#39;speechiness&#39;: 0.0403, &#39;acousticness&#39;: 0.0183, &#39;instrumentalness&#39;: 0, &#39;liveness&#39;: 0.174, &#39;valence&#39;: 0.728, &#39;tempo&#39;: 160.139, &#39;type&#39;: &#39;audio_features&#39;, &#39;id&#39;: &#39;2oPCR02nduYVpj7CTfsA0d&#39;, &#39;uri&#39;: &#39;spotify:track:2oPCR02nduYVpj7CTfsA0d&#39;, &#39;track_href&#39;: &#39;https://api.spotify.com/v1/tracks/2oPCR02nduYVpj7CTfsA0d&#39;, &#39;analysis_url&#39;: &#39;https://api.spotify.com/v1/audio-analysis/2oPCR02nduYVpj7CTfsA0d&#39;, &#39;duration_ms&#39;: 212067, &#39;time_signature&#39;: 4 } . We can see some metadata like the urls of the song and of this analysis, the track id and then the interesting part: the audio features. . It&#39;s important to know what these features actually represent. You can find the full list on the Spotify website. I&#39;ve included the relevant description in the sections below for each audio features that we will discuss. . If you want to see how you can retrieve this information from Spotify, please check the collapsed cell below. . #collapse-hide retrieve_audio_features = False # programmical toggle for easier working. Please ignore. if retrieve_audio_features: track_features_list = [] import math for i in tqdm(range(len(track_ids))): if track_ids[i] == &#39;&#39;: track_features_temp = None if isinstance(track_ids[i], float): if math.isnan(track_ids[i]): print(&#39;isnan&#39;) track_features_temp = None else: try: track_features_temp = spotify.audio_features(track_ids[i]) except Exception as e: print(e) print(track_ids[i]) track_features_list += track_features_temp if track_features_temp else [{}] track_features = pd.DataFrame(track_features_list) . . We combine the data, resulting in a single dataframe that holds all our data: song metadata, audio features and genres. . # Add our audio features with our track information comb = pd.concat([track_info, track_features], axis=1) # Also add the top genres of each song comb[&#39;top_1_genres&#39;] = top_1_genres.top_1_genres comb[&#39;top_1_genres_filtered&#39;] = top_1_genres_filtered.top_1_genres_filtered . Drop the songs without audio features. We drop on acousticness, but it could be any other audio features, since a song either has all of them, or none of them. . comb = comb.dropna(subset=[&#39;acousticness&#39;]) . The Spotify website also includes distribution plots for most features, so we will take at how you might differ from the general plots and how you can see what causes this. I have selected a subset of audio features, but you can evaluate the rest yourself. Looking at the values they report, their analysis seems to have been done on about 10k data points. To get a better comparison, we will sample 10k data points and compare with that. However, when diving deeper into something interesting, we use the full dataset. In all of the distribution comparisons below, the first plot is from Spotify and the second one from me. Additionally, under each feature I put the description Spotify gives for those features. . Danceability . Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. . Here, we take a look at danceability. Given my main genres, we should see higher danceability than normally, with EDM being a big part of my listening. And indeed we do see this. In my distribution, we see quite some peaks around 0.8, while the general distribution has only a very small amount of songs with danceability higher than 0.8. Since we only have an image from spotify (and no actual data points), we cannot compare means, but I expect the general distribution mean to be somewhere between 0.5-0.6, while mine is 0.67 . . Energy . Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. . Now, energy is a bit strange. In my year in review, Spotify always telling me I listen to so much energetic music. However, looking at this plot, I seem to be way lower than the general distribution. . . Let&#39;s see if we get some more clarity if we look at the energy per artist. . Important: For this table and all that follow, the count is just the frequency count of that value, whatever that value might be. If it is an artist, it is the total number of times that artist was played. If it is a song, it is the number of times that song was played. . energy . count mean . master_metadata_album_artist_name . 6ix9ine 260 | 0.699400 | . KIDS SEE GHOSTS 302 | 0.691255 | . Fresku 264 | 0.657288 | . Tchami 370 | 0.644843 | . Netsky 466 | 0.639095 | . Kid Cudi 853 | 0.634662 | . Kendrick Lamar 2740 | 0.633111 | . Mac Miller 527 | 0.628738 | . Flume 593 | 0.627781 | . Travis Scott 265 | 0.623781 | . Yellow Claw 2533 | 0.618704 | . Various Artists 1594 | 0.618487 | . Eminem 1567 | 0.616364 | . Yung Internet 373 | 0.607120 | . G-Eazy 926 | 0.605303 | . Lil Dicky 303 | 0.588901 | . Kanye West 1070 | 0.571160 | . Parov Stelar 264 | 0.563025 | . Vitalic 275 | 0.543753 | . Frank Sinatra 804 | 0.512106 | . These results I find quite surprising. Yellow Claw, a Dutch trap duo whom I would consider to be generally pretty high energy is equal or below quite some hip hop artists, like Mac Miller and Kid Cudi. We can see the maximum energy in here is 6ix9ine, with a solid 0.69 😂. There is no denying that 6ix9ine is at that level of energy. However, KIDS SEE GHOSTS, the front for Kid Cudi and Kanye West, somehow also has 0.69, while I&#39;d say that is much lower in energy. If we look at Reborn, the most popular song on the album, it&#39;s clearly a pretty slow song. . Alternatively, we can look at the energy of my top songs. Here we get a clearer picture of the effect of different songs. . energy . count mean . master_metadata_album_artist_name master_metadata_track_name . Kendrick Lamar HUMBLE. 125 | 0.6210 | . DNA. 124 | 0.5230 | . ELEMENT. 93 | 0.7050 | . LOYALTY. FEAT. RIHANNA. 81 | 0.5350 | . YAH. 81 | 0.7000 | . FEEL. 80 | 0.7950 | . Travis Scott goosebumps 99 | 0.7280 | . Yellow Claw City on Lockdown (feat. Juicy J &amp; Lil Debbie) 96 | 0.8410 | . Good Day (feat. DJ Snake &amp; Elliphant) 87 | 0.5660 | . Open (feat. Moksi &amp; Jonna Fraser) 87 | 0.5920 | . Without You (feat. The Galaxy &amp; Gia Koka) 82 | 0.4880 | . Invitation (feat. Yade Lauren) 80 | 0.6620 | . Love &amp; War (feat. Yade Lauren) 78 | 0.5330 | . Last Paradise (feat. Sody) 78 | 0.5930 | . Stacks (feat. Quavo, Tinie Tempah &amp; Cesqeaux) 77 | 0.0812 | . There are some pretty interesting facts here. First of, we can see see that my idea of Yellow Claw being very high energy is debunked, at least with Spotify&#39;s definition of energy. Because some of these values are very strange, even more so when compared to some other artists and songs. Take for example the song without you from Yellow Claw. This is a dubstep/brostep song, with a pretty high tempo but has a value of 0.4880 for its energy level. This is lower than Frank Sinatra had on average in the previous table. In my opinion, something is wrong there. Furthermore, we can also see that their song Stacks has an energy value of 0.0812, which, by all accounts, should be an anomaly. My expectation is that they trained a neural network with some hand labeled songs, and then applied that to all songs to estimate these values, and something went wrong in the case of this song. . Instrumentalness . Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. . . This is potentially interesting, since, naturally, hip-hop has a lot of vocals (so low instrumentalness), but genres like electro house and EDM generally don&#39;t have many vocals, so how is this plot explained with almost nothing around 1. We can quickly see why by looking at the most influential artists for the largest genres. I&#39;m showing the top 10 most played artists with their largest genre (e.g. if an artist is tagged as &#39;rap&#39; and &#39;west coast hip hop&#39;, we use the more general rap) . count . top_1_genres master_metadata_album_artist_name . edm Yellow Claw 2531 | . Flume 592 | . Tchami 370 | . rap Kendrick Lamar 2738 | . Eminem 1564 | . Kanye West 1065 | . G-Eazy 926 | . Kid Cudi 852 | . Mac Miller 527 | . Lil Dicky 303 | . As most influential EDM artists, we see Yellow Claw and Flume, both of which have a lot of vocals in their music. This explains our instrumentalness plot!. . However, if we redo this analysis, but we drop the most general genres, we get much more interesting results. To be specific, I&#39;ve ignored the values pop, edm, rap, pop rap and hip hop, unless there were no other genre labels. In that case, we still take that genre. The result is below, and quite interesting. The table shows the top 20 most played artists and the genre they belong to. . master_metadata_album_artist_name . top_1_genres_filtered master_metadata_album_artist_name . chicago rap Kanye West 1065 | . christmas Frank Sinatra 804 | . conscious hip hop Kendrick Lamar 2738 | . detroit hip hop Eminem 1564 | . downtempo Flume 592 | . Parov Stelar 263 | . dutch hip hop Yung Internet 373 | . Fresku 263 | . electro house Yellow Claw 2531 | . Tchami 370 | . Vitalic 274 | . emo rap 6ix9ine 260 | . house FISHER 247 | . indie pop rap G-Eazy 926 | . liquid funk Netsky 466 | . rap Kid Cudi 852 | . Mac Miller 527 | . Lil Dicky 303 | . KIDS SEE GHOSTS 302 | . Travis Scott 264 | . Without the super general genres, we can finally start to see some trends. Given the fact that rap is still the main genre of 5 artists, this often seems to be one of few labels applied, possibly with the other labels I was removing. The results is that these artists do not belong to any specific subgenres within hip hop, which is cool to see. Does this also mean they are per definition mainstream? . Note: In part 2, the question arose of why emo rap was one of my main genres in some months. This is also explained by this plot, because 6ix9ine is regarded as emo rap by spotify, and his presence in my listening is very varied, with sometimes none and sometimes quite a bit. . loudness . The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db. . . We can see my music is quite a bit louder than average, with almost all of the mass of the distribution being between -10 and 0, with a small tail between -15 and -10. In the general distribution, the tail extends quite a bit beyond -20, whereas there is virtually nothing beyond -18 in my distribution. Let&#39;s see who is responsible for all of this noise 😠. Let&#39;s plot the loudness of my top 20 most listened artists. . loudness . count mean . master_metadata_album_artist_name . 6ix9ine 260 | -5.624454 | . Travis Scott 265 | -5.832879 | . Kid Cudi 853 | -6.496838 | . Kanye West 1070 | -6.535216 | . KIDS SEE GHOSTS 302 | -6.612129 | . Fresku 264 | -6.614091 | . Netsky 466 | -6.690002 | . G-Eazy 926 | -6.849693 | . Yellow Claw 2533 | -6.875627 | . Kendrick Lamar 2740 | -7.033027 | . Eminem 1567 | -7.094228 | . Various Artists 1594 | -7.189992 | . Tchami 370 | -7.304330 | . Flume 593 | -7.348245 | . Mac Miller 527 | -7.576696 | . Parov Stelar 264 | -7.825705 | . Yung Internet 373 | -7.844906 | . Vitalic 275 | -8.272200 | . Frank Sinatra 804 | -8.528413 | . Lil Dicky 303 | -9.120274 | . This table is pretty clear. We see that none of my top 20 most played artists have an average loudness lower than -10, indicating quite high loudness on average. If we look at the top 5, we see that apparently hip hop artists are very loud, but also specifically Kid Cudi, Kanye West, and then those two combined in KIDS SEE GHOSTS. The reason for this I expect to be that they have sounds playing at almost at all times, rather than that they are extremely loud in their peaks. . It also comes as no surprise that 6ix9ine is the loudest, since he&#39;s essentially screaming in most of his songs 😅. . valence . A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). . This is a very interesting feature. Let&#39;s see what information it gives us! . . That&#39;s quite the difference! First off, we can see a clear decline towards 1.0 for my distribution, whereas in the general distribution, this is much rounder. Furthermore, we can see that my distribution a lot spikier, possibly indicating the effects of certain albums/songs that have been played a lot and have a very narrow spread with regards to valence. In general we can see my music taste leans more towards the low side of valence, indicating a preference for sad, depressed and angry music. In general, I think it will be angry, which is the sentiment in a lot of hip hop, but also techno, drum and bass and some parts of house. . If we look at the most positive and most negative songs with more than 5 plays, we see some interesting results. . valence . count mean . master_metadata_album_artist_name master_metadata_track_name . Lil Dicky White Crime 19 | 0.00000 | . Eminem White America 31 | 0.00000 | . Kanye West Fade 37 | 0.00000 | . Ellen Allien Stormy Memories 10 | 0.00001 | . Hollen Sleeping Dogs - Original 11 | 0.01400 | . ... ... ... | ... | . Ed Sheeran Shape of You 9 | 0.93100 | . Fresku Chickie (skit) 15 | 0.93800 | . Victor Ruiz Brujeria 7 | 0.96000 | . Kendrick Lamar Momma 36 | 0.96300 | . Yung Internet Helemaal Top (feat. Donnie) 48 | 0.96300 | . 1171 rows × 2 columns . We can see that there are likely some errors in here, with White Crime and White America both having a score of 0.0. On the positive side, we see Yung Internet with Helemaal Top, which is a song about feeling great. Furthermore, we have Kendrick Lamar&#39;s Momma. This is interesting cause it does not typically resonate as a very happy or cheerful song. We&#39;ll attribute this to the Spotify interpretation of this value. See for yourself: If we take a higher level view, we can look at the artists and identify more broader trends. . valence . count mean . master_metadata_album_artist_name . Kanye West 1070 | 0.327687 | . Tchami 370 | 0.360095 | . Netsky 466 | 0.383689 | . KIDS SEE GHOSTS 302 | 0.414606 | . Yellow Claw 2533 | 0.430344 | . Frank Sinatra 804 | 0.434713 | . Lil Dicky 303 | 0.445153 | . G-Eazy 926 | 0.445842 | . Mac Miller 527 | 0.462410 | . Kendrick Lamar 2740 | 0.469246 | . Various Artists 1594 | 0.476991 | . Kid Cudi 853 | 0.486193 | . Eminem 1567 | 0.493737 | . Yung Internet 373 | 0.511973 | . Flume 593 | 0.575045 | . Some interesting insights: . Kanye is depressing: Kanye West is very low in valence, meaning most of his songs are angry, depressed or sad. This makes sense, given his oeuvre, with songs like Waves (sad), Violent Crimes (sad), Piss On Your Grave (angry) and I Am A God (angry). | Bias of electronic music: We have some electronic artists like Tchami and Netsky which also rank very low, but which are not specifically angry or sad music producers in my experience. Maybe electronic music has a bias here and is faster to be considered angry or sad? | Non-polarity: We see quite some artists hovering around 0.5, indicating either a healthy balance in valence between their songs or just a general non-polarity in their songs. I took a detailed look at Kanye West for the first point, and he has a wide spread, with the weight more on the sad and angry side, hence his low average valence. I assume most artists will be similar, and have a wide spread between their songs. | Audio features per Genre &#127928; . Apart from the details per feature, we can also take a look at how these distributions are affected by genres. We&#39;ll cherry-pick some interesting ones. . Energy in Rap and EDM . . Important: From here one out, all plots are again interactive. Try to disable some things in the legend! Given our results in previous sections, one of the things I&#39;d like to look deeper into is the effects of EDM and rap with regards to energy. . Above, we see the Kernel Density Estimation (KDE) of energy for rap and EDM. Rap is strange, since it seems to have two distinct peaks; one around 0.6 and one around 0.75. EDM has a more well defined single peak at 0.6. Furthermore, we see that EDM is more prevalent in the high energy values, which is not too unsurprising to see. Let&#39;s dive a bit deeper into the two peaks of rap. . If we plot different hip hop subgenres, we can see how these peaks come to be. We plot the largest three hip hop subgenres. Detroit hip hop seems to have more of a peak around 0.75, while conscious hip hop and chicago rap have peaks around at 0.6. It seems that they have a lot more energy in Detroit! . Valence in Rap and EDM . We can see some interesting differences in valence for these genres. We observe that EDM is more prevalent in the low valence values (0.0-0.2), dropping to zero much later than rap. Rap, on the other hand, is more prevalent in the 0.3-0.5 range than EDM. Now, we still don&#39;t know if 0.5 valence means it is neutral, but it can tell us at least that rap is on average less sad and angry than EDM, which I find an interesting discovery, since I would have guessed it the other way around. . Audio features against each other &#9876;&#65039; . To find interesting relationships between these audio features, we can create a pairplot, that create a scatterplot for each two variables. Because the calculation time for the number of data points times the number of plots is very high, we sample the dataset. This should allow us to visualize the distributions in general sense. We don&#39;t require each individual data point here. . We sample 2000 data points at random. We also filter on the two largest genres, EDM and rap, so we can see some differences between these genres. . Thats a pretty big plot with a lot of information. Lets see if we can find some interesting insights. . Distribution differences between rap and EDM: We see that in the outliers, there are generally many more EDM songs than rap songs. This might indicate that in EDM, there is more variation in these aspects. This phenomenon is visible in the instrumentalness plots, because in this category, almost all of the outliers are in EDM. But this is also the case for valence vs loudness, energy vs danceability, loudness vs danceability and some others. We see that several variables are returning, meaning that there&#39;s a distribution difference in those marginal distributions. | Outliers: We can see two outliers in many plots, being Sound of Walking Away from Illenium, and White Iverson from Post Malone. This is because Sound of Walking Away is just incredibly low on loudness. White Iverson is more interesting, because it is still part of the general distribution in loudness, albeit on the edge. But the combination of its features is just often very unique, like in loudness vs instrumentalness, instrumentalness vs energy and instrumentalness vs danceability. If compared to just the rap tracks, it&#39;s also an outlier in danceability vs acousticness and acousticness vs loudness. | Linear relationships: We can also identify some roughly linear relationships. Acousticness and Loudness: As acousticness increases, loudness seems to decrease, especially near the high acousticness values. | Energy vs loudness: As the energy of a song increases, so does it loudness often. This is not a huge surprise, given the songs that are used for dancing. | Energy vs Acousticness: Unsurprisingly, given the two former points, there is also a relationship visible between energy and acousticness, where the energy decreases with the increase of acousticness. | . | I&#39;ve also created the large version of above plot, with more columns and more datapoints. See it here big plot . Audio features over time . As a last area of research, lets have a look at how some of these features might have changed over time in my listening behaviour. . The last thing we&#39;ll take a look at is, like with the genres, whether my preference for these audio features changed over time. I&#39;ve plotted the ones with the same range, so loudness, tempo and duration are excluded. For completeness, I looked at loudness but there were no interesting insights there. In the chart above, we can see a couple interesting things. . General increase in stability: We can observe an increase in stability for most of the features, likely explained due to the increased stability in genres and the higher listening volume. | Deccrease in Acousticness: Furthermore, we can see a decrease in acousticness. The reduction in acousticness is expected with hip hop and EDM becoming the main genres. | A turbulent start: Lastly, we see that the first 6 months were quite turbulent, but this is likely related with the fact that we don&#39;t have that much data for those months. | In general, no big revelations from this plot! We can conclude that no big changes have happened with regards to the features of the audio. . In conclusion . In part 3 we have taken a closer look at the audio features that are in Spotify data, specifically for my listening. We&#39;ve seen what the effects of EDM and rap are on certain audio features For rap, we&#39;ve also seen how it&#39;s distribution is built up from the distributions of the subgenres. Lastly, we&#39;ve also seen how most of the audio features relate to other features, and what songs are outliers in those distributions. . With regards to my initial goal: to track what I&#39;m doing (and also learn about myself), this part was a bit harder. The audio features are not always intuitive to interpret and can differ from what your expectations are. They are also a lot less interesting in daily life, whereas favorite artists is still quite fun to know (in my opinion). Nevertheless, it was quite interesting to see what the effects of genres and subgenres were on the global distribution. . For now, that&#39;s it! I&#39;ve seen enough Spotify data and I&#39;m looking for something new. . If you liked this blogpost, don&#39;t hesitate to reach out to me on linkedin or twitter. 😊 . .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/spotify-listening-history-analysis-part-3.html",
            "relUrl": "/posts/spotify-listening-history-analysis-part-3.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Analyzing my Spotify listening history 🎵 - Part 2",
            "content": "Short recap: Part 1 . In part 1 of this series we looked at the first part of this project. This included: . The data we are working with and what it looks like. | The amount of listening done per year and per month. | The amount of listening done per hour of day, also throughout the years. | The amount of genres we have per song/artist. | We will continue from where we left of, diving deeper into the genres. . We&#39;ll load up the original JSON from Spotify, as well as the genres we created in part 1. We then combine them into comb, the combined dataframe. In genres.csv, we again see the 20 columns with the genres for each song, where the genres are collected from the artist, since songs are not labeled as having a genre. For more details, please have a look at part 1. . # data received from Spotify df.head(1) . ts ms_played conn_country master_metadata_track_name master_metadata_album_artist_name master_metadata_album_album_name reason_start reason_end shuffle skipped ... city region episode_name episode_show_name date year month day dow hour . 0 2013-10-09 20:24:30+00:00 | 15010 | NL | Wild for the Night (feat. Skrillex &amp; Birdy Nam... | A$AP Rocky | LONG.LIVE.A$AP (Deluxe Version) | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | 2013-10-09 | 2013 | 10 | 9 | 2 | 20 | . 1 rows × 25 columns . Genres retrieved from Spotify and the combined dataframe. We rename the genres columns from just a number 0-20 to &#39;genre_x&#39; with x between 0 and 20, so they&#39;re easier to recognize. . comb consists of df + genres_df, with the genre columns at the end. . # genres retrieved through Spotify API genres_df = pd.read_csv(&#39;genres.csv&#39;, low_memory=False) genres_df = genres_df.rename(columns={str(x): f&#39;genre_{x}&#39; for x in range(21)}) comb = pd.concat([df, genres_df], axis=1) comb.head(2) . ts ms_played conn_country master_metadata_track_name master_metadata_album_artist_name master_metadata_album_album_name reason_start reason_end shuffle skipped ... genre_11 genre_12 genre_13 genre_14 genre_15 genre_16 genre_17 genre_18 genre_19 genre_20 . 0 2013-10-09 20:24:30+00:00 | 15010 | NL | Wild for the Night (feat. Skrillex &amp; Birdy Nam... | A$AP Rocky | LONG.LIVE.A$AP (Deluxe Version) | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 2013-10-09 20:19:20+00:00 | 68139 | NL | Buzzin&#39; | OVERWERK | The Nthº | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 rows × 46 columns . Top genres . In part 1, we have seen how many genres each song has and how their numbers are distributed. The next question then, naturally, is: What genres are they? So let&#39;s see! . For the following analyses, remember that if I play 10 songs by Kanye, Kanye&#39;s genres will be present 10 times. . To analyze the genres, I first create a dataframe that contains all of the genres and their counts. This will be handy in the near future. . top_genres = ( genres_df.apply(pd.Series.value_counts) .apply(np.sum, axis=1) .sort_values(ascending=False) .reset_index() .rename(columns={&#39;index&#39;: &#39;genre&#39;, 0: &#39;count&#39;}) ) . Then we can plot. Lets start with the total listens per genre. . No big surprises here. My main music tastes are hip hop and electronic music, with main genres techno and drum and bass. However, for the latter two I mainly use youtube, which hosts sets that Spotify does not have. So my Spotify is mainly dominated by hip hop and its related genres, like rap, hip hop and pop rap (whatever that is? Drake maybe?). I expect many hip hop songs are also tagged as pop, which would explain the high pop presence, while I normally am not such a pop fan. Lets dive a bit deeper into this! . As a next step, let&#39;s verify which genres coincide with which other genres. This will test our hypothesis that pop is used as a tag for hip hop, but will also in general provide us with a better feeling of what genres are related to which other genres. . For this,we loop over the rows and for each present genre, we put a 1 in that column, while also casting to np.int8. This means that, instead of the normally 32 bits, we use 8 bits and thus safe some memory. Since we only wanna represent a binary state (present or not present), we could also use boolean. However, since we&#39;re doing arithmetic with it later, int8 will do. We fill the empty cells with 0. We only do this for the top 20 genres. This results in a dataframe with a column for each of the top 20 genres. . rows = [] for i, row in comb.loc[:, [f&#39;genre_{x}&#39; for x in range(21)]].iterrows(): new_row = {} for value in row.values: if value in top_genres_20: new_row[value] = 1 rows.append(new_row) genre_presence = pd.DataFrame(rows) genre_presence = genre_presence.fillna(0).astype(np.int8) genre_presence.head(2) . hip hop pop pop rap rap edm electro house dance pop tropical house big room brostep bass trap electronic trap house progressive electro house progressive house detroit hip hop g funk west coast rap conscious hip hop tech house . 0 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Now that we have this data, we can do a correlation analysis of when each genre coincides with what other genre. Now, because genre is a nominal data type, we cannot use the standard correlation, which is the Pearson correlation coefficient. Instead, we should use a metric that works with nominal values. I choose Kendall&#39;s tau for this, due to its simplicity. Normally, Kendall&#39;s tau is meant for ordinal values (variables that have an ordering). However, because we are working with a binary situation (genre is either present or not) represented by 0 and 1, I think this should still work. One other thing to note is that Kendall&#39;s tau is symmetric, and this means tau(a, b) is the same as tau(b, a). . Note: If you have thoughts on how to do this better, let me know cause I&#8217;m definitely open for ideas. 😉 . Lets loop over all the combinations of the top 20 genres and compute their tau coefficient. . from scipy.stats import kendalltau from itertools import product rows = [] for genre_a, genre_b in product(genre_presence.columns.values, repeat=2): tau, p = kendalltau(genre_presence[genre_a].values, genre_presence[genre_b].values) rows.append({&#39;genre_a&#39;: genre_a, &#39;genre_b&#39;: genre_b, &#39;tau&#39;: tau}) tau_values = pd.DataFrame(rows) tau_values[:2] . genre_a genre_b tau . 0 hip hop | hip hop | 1.000000 | . 1 hip hop | pop | -0.040954 | . For each combination of the top 20 genres, we now know how they are correlated and how much. We can make a nice correlation dataframe from this using the command below. However, because Altair wants data in long format, I won&#39;t use that for the visualization. Furthermore, it is very large and won&#39;t fit neatly inside the blog :wink:. . corr = tau_values.pivot(index=&#39;genre_a&#39;, columns=&#39;genre_b&#39;, values=&#39;tau&#39;).fillna(0).style.background_gradient(cmap=&#39;coolwarm&#39;, axis=None) corr . genre_b bass trap big room brostep conscious hip hop dance pop detroit hip hop edm electro house electronic trap g funk hip hop house pop pop rap progressive electro house progressive house rap tech house tropical house west coast rap . genre_a . bass trap 1.000000 | -0.047263 | 0.827810 | -0.090433 | -0.073662 | -0.064127 | 0.548764 | 0.609779 | 0.829662 | -0.063805 | -0.170851 | -0.094435 | 0.435262 | -0.165831 | -0.061091 | -0.074626 | -0.191328 | -0.067368 | -0.088661 | -0.085121 | . big room -0.047263 | 1.000000 | 0.054333 | -0.075775 | 0.388532 | -0.053733 | 0.476109 | 0.459992 | -0.021669 | -0.053463 | -0.143158 | 0.229690 | 0.118089 | -0.137685 | 0.813625 | 0.593826 | -0.160316 | 0.027803 | 0.479743 | -0.071324 | . brostep 0.827810 | 0.054333 | 1.000000 | -0.102472 | -0.036446 | -0.072664 | 0.610668 | 0.684226 | 0.858659 | -0.072300 | -0.193596 | -0.012040 | 0.369085 | -0.187908 | 0.037453 | -0.039508 | -0.216799 | -0.059485 | -0.076292 | -0.096453 | . conscious hip hop -0.090433 | -0.075775 | -0.102472 | 1.000000 | -0.072027 | -0.062703 | -0.156183 | -0.140868 | -0.106408 | -0.057746 | 0.508987 | -0.092338 | -0.108786 | 0.541556 | -0.064083 | -0.072969 | 0.469347 | -0.065872 | -0.087683 | 0.906393 | . dance pop -0.073662 | 0.388532 | -0.036446 | -0.072027 | 1.000000 | -0.051075 | 0.314206 | 0.185610 | -0.019590 | -0.050819 | -0.112595 | 0.009366 | 0.370197 | -0.095763 | 0.322183 | 0.138411 | -0.135577 | -0.053657 | 0.519990 | -0.067796 | . detroit hip hop -0.064127 | -0.053733 | -0.072664 | -0.062703 | -0.051075 | 1.000000 | -0.110751 | -0.099891 | -0.075455 | 0.940323 | 0.375338 | -0.065478 | -0.079217 | -0.083439 | -0.045442 | -0.051743 | 0.334888 | -0.046711 | -0.062177 | -0.057561 | . edm 0.548764 | 0.476109 | 0.610668 | -0.156183 | 0.314206 | -0.110751 | 1.000000 | 0.793586 | 0.638941 | -0.110196 | -0.295071 | 0.233739 | 0.506101 | -0.283308 | 0.387752 | 0.444994 | -0.323154 | -0.067377 | 0.444996 | -0.147009 | . electro house 0.609779 | 0.459992 | 0.684226 | -0.140868 | 0.185610 | -0.099891 | 0.793586 | 1.000000 | 0.691817 | -0.099390 | -0.266136 | 0.268155 | 0.334471 | -0.257533 | 0.429638 | 0.416084 | -0.298033 | -0.052833 | 0.283883 | -0.132593 | . electronic trap 0.829662 | -0.021669 | 0.858659 | -0.106408 | -0.019590 | -0.075455 | 0.638941 | 0.691817 | 1.000000 | -0.075077 | -0.201032 | 0.039264 | 0.372795 | -0.195125 | -0.052922 | 0.058709 | -0.225126 | -0.062905 | 0.060665 | -0.100158 | . g funk -0.063805 | -0.053463 | -0.072300 | -0.057746 | -0.050819 | 0.940323 | -0.110196 | -0.099390 | -0.075077 | 1.000000 | 0.370783 | -0.065150 | -0.108395 | -0.090262 | -0.045214 | -0.051484 | 0.330955 | -0.046477 | -0.061865 | -0.014741 | . hip hop -0.170851 | -0.143158 | -0.193596 | 0.508987 | -0.112595 | 0.375338 | -0.295071 | -0.266136 | -0.201032 | 0.370783 | 1.000000 | -0.173608 | -0.040954 | 0.767805 | -0.121069 | -0.137857 | 0.869137 | -0.124450 | -0.164775 | 0.495227 | . house -0.094435 | 0.229690 | -0.012040 | -0.092338 | 0.009366 | -0.065478 | 0.233739 | 0.268155 | 0.039264 | -0.065150 | -0.173608 | 1.000000 | -0.080532 | -0.167398 | 0.289283 | 0.479905 | -0.194561 | 0.188667 | 0.334118 | -0.086915 | . pop 0.435262 | 0.118089 | 0.369085 | -0.108786 | 0.370197 | -0.079217 | 0.506101 | 0.334471 | 0.372795 | -0.108395 | -0.040954 | -0.080532 | 1.000000 | 0.011708 | 0.041644 | 0.035270 | 0.005871 | -0.114448 | 0.229243 | -0.144607 | . pop rap -0.165831 | -0.137685 | -0.187908 | 0.541556 | -0.095763 | -0.083439 | -0.283308 | -0.257533 | -0.195125 | -0.090262 | 0.767805 | -0.167398 | 0.011708 | 1.000000 | -0.117512 | -0.133807 | 0.851939 | -0.120794 | -0.158329 | 0.507457 | . progressive electro house -0.061091 | 0.813625 | 0.037453 | -0.064083 | 0.322183 | -0.045442 | 0.387752 | 0.429638 | -0.052922 | -0.045214 | -0.121069 | 0.289283 | 0.041644 | -0.117512 | 1.000000 | 0.511875 | -0.135579 | 0.051915 | 0.378595 | -0.060318 | . progressive house -0.074626 | 0.593826 | -0.039508 | -0.072969 | 0.138411 | -0.051743 | 0.444994 | 0.416084 | 0.058709 | -0.051484 | -0.137857 | 0.479905 | 0.035270 | -0.133807 | 0.511875 | 1.000000 | -0.154379 | 0.051001 | 0.492784 | -0.068683 | . rap -0.191328 | -0.160316 | -0.216799 | 0.469347 | -0.135577 | 0.334888 | -0.323154 | -0.298033 | -0.225126 | 0.330955 | 0.869137 | -0.194561 | 0.005871 | 0.851939 | -0.135579 | -0.154379 | 1.000000 | -0.139366 | -0.184675 | 0.442063 | . tech house -0.067368 | 0.027803 | -0.059485 | -0.065872 | -0.053657 | -0.046711 | -0.067377 | -0.052833 | -0.062905 | -0.046477 | -0.124450 | 0.188667 | -0.114448 | -0.120794 | 0.051915 | 0.051001 | -0.139366 | 1.000000 | -0.015924 | -0.062003 | . tropical house -0.088661 | 0.479743 | -0.076292 | -0.087683 | 0.519990 | -0.062177 | 0.444996 | 0.283883 | 0.060665 | -0.061865 | -0.164775 | 0.334118 | 0.229243 | -0.158329 | 0.378595 | 0.492784 | -0.184675 | -0.015924 | 1.000000 | -0.082532 | . west coast rap -0.085121 | -0.071324 | -0.096453 | 0.906393 | -0.067796 | -0.057561 | -0.147009 | -0.132593 | -0.100158 | -0.014741 | 0.495227 | -0.086915 | -0.144607 | 0.507457 | -0.060318 | -0.068683 | 0.442063 | -0.062003 | -0.082532 | 1.000000 | . A much better approach is using Altair, so let&#39;s see how these genres correlate then. . We immediately can see some interesting clusters. We can see a strong tau between most of the electronic music genres, like edm, electro house, bass trap, big room, brostep and electronic trap. Then, looking at hip hop, we can see very strong coefficients with rap and pop rap, neither of which are big suprises. My initial hypothesis that pop would be correlated with hip hop has been debunked, though. Pop seems to be more strongly related with edm ($+0.51$) and some other electronic genres, and have a negative tau with hip hop related genres, like hip hop ($-0.29$), pop rap ($-0.28$) and rap ($-0.32$). . In this overview, I think there are two interesting insights still: . A strong coefficient between conscious hip hop and west coast rap. I did not really expect this, but can likely be attributed to artists like Kendrick Lamar, who deal with social and political issues in their lyrics. Additionally, cities like Compton played a big role in west coast hip hop, and were often strongly related to their social and economical situation (Also for Kendrick Lamar). | A strong coefficient between G-funk and Detroit hip hop. G-funk is a is a subgenre of hip hop that originated in the west coast, while Detroit hip hop, as the name says, comes from Detroit. A strong coefficient between G-funk and west coast rap might have been more expected. Interesting to see, but I won&#39;t dive deeper into these findings for now. | . Monthly change in genres &#128197; . This is a very interesting analysis in my opinion, but also one of the more challenging one. I&#39;ve approached the problem the following way, given the data I had. . Count the frequency of each genre on a certain interval, monthly in this case. | Divide these numbers by the total plays for those intervals, so we get a percentage of total plays of that month. This number means how much of the songs had that genre. This means that these percentages will not sum to one (or you know, they can, but they don&#39;t have to). | Sort given these percentages and extract the monthly top 5. | Step 1: count the frequency per interval. We don&#39;t do this just for the top $n$ genres, but for all genres. This, naturally, results in a lot of columns and a very wide dataframe. . # Step 1. Count all genre occurences per month. counters_per_month = [] unique_years = comb.year.sort_values().unique() unique_months = comb.month.sort_values().unique() for year, month in tqdm(product(unique_years, unique_months), total=len(unique_years)*len(unique_months)): if len(comb.loc[(comb.year == year) &amp; (comb.month == month)]) &gt; 0: counter = {&#39;year&#39;: year, &#39;month&#39;: month} for i, row in comb.loc[(comb.year == year) &amp; (comb.month == month)].iterrows(): for genre in row[[f&#39;genre_{x}&#39; for x in range(21)]]: # the genre columns are named &#39;0&#39; to &#39;20&#39;. counter[genre] = counter.get(genre, 0) + 1 counters_per_month.append(counter) . Put the counts_per_month in a dataframe and calculate the total songs played per month. . counts_per_genre_per_month = pd.DataFrame(counters_per_month) monthly_sum = df.groupby([&#39;year&#39;, &#39;month&#39;]).size().reset_index().rename(columns={0: &#39;count&#39;}) . Step 2: We then normalize all genre counts by the number of songs played in that time period. . # 2.Normalize all genre counts by the number of songs played in that time period. # Select all columns except the time columns columns = counts_per_genre_per_month.columns.tolist() columns.remove(&#39;year&#39;) columns.remove(&#39;month&#39;) for i, row in monthly_sum.iterrows(): counts_per_genre_per_month.loc[(counts_per_genre_per_month.year == row.year) &amp; (counts_per_genre_per_month.month == row.month), columns] = counts_per_genre_per_month.loc[(counts_per_genre_per_month.year == row.year) &amp; (counts_per_genre_per_month.month == row.month), columns] / row[&#39;count&#39;] . To get a cleaner visual, we remove any data before August 2016. . counts_per_genre_per_month_filtered = counts_per_genre_per_month.loc[(counts_per_genre_per_month.year &gt; 2016) | ((counts_per_genre_per_month.year == 2016) &amp; (counts_per_genre_per_month.month &gt; 8))] . We now have a dataframe with 863 columns, which corresponds to 861 different genres. This dataframe has all the genres and what percentage of total plays they were present as a genre. Keep in mind that an artist/song generally has more than one genre, so the sum of these fractions is not 1. This dataframe looks like this: . year month east coast hip hop hip hop pop pop rap rap trap music NaN catstep ... classical soprano spanish hip hop trap espanol pop reggaeton chinese hip hop corrido regional mexican pop australian indigenous witch house ghettotech . 16 2016 | 9 | 0.038760 | 0.449612 | 0.387597 | 0.488372 | 0.519380 | 0.069767 | 16.689922 | 0.007752 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 17 2016 | 10 | 0.055409 | 0.313984 | 0.343008 | 0.279683 | 0.337731 | 0.036939 | 16.469657 | 0.026385 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 rows × 863 columns . Step 3: Sort given these values and extract the top 5. Unfortunately, the data is not in a shape that we can do that (to my knowledge at least), so we need to transform it a bit further by moving from a wide to a long data format and filtering out some values. . The melting of the dataframe results in a single row per percentage per genre per timeunit. This makes it easier to plot with Altair. Furthermore, we create a datetime column from our year + month columns, which is also better for Altair to use. . counts_per_genre_per_month_melted = pd.melt(counts_per_genre_per_month_filtered, id_vars=[&#39;year&#39;, &#39;month&#39;], value_vars=columns, var_name=&#39;genre&#39;, value_name=&#39;percentage&#39;) counts_per_genre_per_month_melted[&#39;datetime&#39;] = pd.to_datetime(counts_per_genre_per_month_melted.month.astype(str) + &#39;-&#39; + counts_per_genre_per_month_melted.year.astype(str), format=&#39;%m-%Y&#39;) . Drop columns where either the genre or percentage is Nan. This reduces the number of rows even more, so that taking the n-largest later will be faster. . counts_per_genre_per_month_melted = counts_per_genre_per_month_melted.dropna(subset=[&#39;percentage&#39;, &#39;genre&#39;]) . year month genre percentage datetime . 0 2016 | 9 | east coast hip hop | 0.038760 | 2016-09-01 | . 1 2016 | 10 | east coast hip hop | 0.055409 | 2016-10-01 | . This looks great! But, there is one problem, and that is that we likely have way too many rows for Altair. . counts_per_genre_per_month_melted.shape . (6735, 5) . Welp, so we have almost 7k rows, while Altair&#39;s maximum is 5k. Not too bad, but we still need to remove a bunch of rows. But that is fine, since we&#39;re only interested in the top 5 of each month anyway. Using .groupby and .nlargest, we can extract this fairly easy. We extract those the indices of the remaining rows and index into the melted dataframe to only have the rows in the top 5 for each month left. . top_genres_per_month_with_perc = counts_per_genre_per_month_melted.loc[counts_per_genre_per_month_melted.groupby([&#39;year&#39;, &#39;month&#39;]).percentage.nlargest(5).reset_index().level_2.values, :] top_genres_per_month_with_perc.set_index([&#39;year&#39;, &#39;month&#39;]).head(5) . genre percentage datetime . year month . 2016 9 rap | 0.519380 | 2016-09-01 | . 9 pop rap | 0.488372 | 2016-09-01 | . 9 hip hop | 0.449612 | 2016-09-01 | . 9 pop | 0.387597 | 2016-09-01 | . 9 indie pop rap | 0.131783 | 2016-09-01 | . top_genres_per_month_with_perc.shape . (145, 5) . And we only have 145 rows left, so we can use it with Altair 😎. . In the chart below, there is a lot going on. On the x-axis we have time while on the y-axis we have the normalized percentages of the top 5 genres. This means that for each month, the top 5 genres&#39; percentages sum to represent 1. This might be hard to grasp, so I&#39;ve put the non-normalized one next to this plot to make the difference clear. Some colors are used twice, but there is no color scheme available in Altair that supports more than 20 colors, so this will have to do for now 😉. You can hover over the bars to get details of those bars and click on legenda items to highlight a genre. . Top genres with percentages &#128202; . #collapse-hide selection = alt.selection_multi(fields=[&#39;genre&#39;], bind=&#39;legend&#39;) normalized = alt.Chart(top_genres_per_month_with_perc).mark_bar().encode( x=alt.X(&#39;yearmonth(datetime):O&#39;, title=&#39;Month per year&#39;), y=alt.Y(&#39;percentage&#39;, stack=&#39;normalize&#39;, title=&#39;Normalized percentage (%)&#39;), color = alt.Color( &#39;genre&#39;, scale=alt.Scale( scheme=&#39;tableau20&#39;, ) ), tooltip=[&#39;genre&#39;, &#39;percentage&#39;, &#39;yearmonth(datetime)&#39;], order=alt.Order( &#39;percentage&#39;, sort=&#39;descending&#39; ), opacity=alt.condition(selection, alt.value(1), alt.value(0.2)) ).properties( title=&#39;Normalized percentage occurences of top 5 genres per month&#39;, width=MAXWIDTH ).add_selection( selection ) non_normalized = alt.Chart(top_genres_per_month_with_perc).mark_bar().encode( x=alt.X(&#39;yearmonth(datetime):O&#39;, title=&#39;Month per year&#39;), y=alt.Y(&#39;percentage&#39;, title=&#39;Percentage (%)&#39;), color = alt.Color( &#39;genre&#39;, scale=alt.Scale( scheme=&#39;tableau20&#39;, ) ), tooltip=[&#39;genre&#39;, &#39;percentage&#39;, &#39;yearmonth(datetime)&#39;], order=alt.Order( &#39;percentage&#39;, sort=&#39;descending&#39; ), opacity=alt.condition(selection, alt.value(1), alt.value(0.2)) ).properties( title=&#39;Percentage occurences of top 5 genres per month&#39;, width=MAXWIDTH ).add_selection( selection ) non_normalized &amp; normalized . . There are definitely some interesting things in theses plots. We can see some consistent attendees that we also saw in the most listened genres in general, so that&#39;s not a big surprise. For example, these include rap, edm and hip hop. . Seasonal effects: What is quite interesting is to see when the very common genres are not dominating the chart, like in December of 2016. Both in November and December of 2016 we see I was in a very strong Christmas mood, with christmas covering 16% of songs in November and 51%(!) in December. The top genres in December are adult standard, easy listening, christmas and lounge. Those definitely are in the same segment, with my listening, so it&#39;s not surprising that those other genres appear alongside Christmas in a heavy Christmas month. This is because my Christmas music is more focused on the 40s and 50s, with artists like Frank Sinatra and Dean Martin, rather than Mariah Carey. We do not see this seasonal effect in 2017 and 2018, but those years my Christmas music urge was just less, so this drop is explainable. Instead of Christmas, in December of 2018 emo rap is in my top 5 genres 🤔. That might be interesting to look at in another blog post. | Electronic periods: Something else that stands out is that there are electronic music periods, like June, July and August of 2017 and January of 2018. However, both edm and electro house are present in essentially each month as high scorers, so I&#39;m definitely a fan in general. But these peak months still stand out. | Rise of Rap: The last thing that is interesting is probably the fact that rap and hip hop have almost exclusively been the top 2 from February 2018 to January 2019. This indicates a move away from the more electronic genres and more towards hip hop. A possible reason for this might be the move towards more set-based plays for electronic music, which are generally not on Spotify, but on platforms like Youtube. Otherwise, it might just be an actual preference shift. However, I do still listen to a lot of these types of music, so I suspect the former. Looking at data from 2019 and 2020 might give some insight in this. | . Top genres without percentages &#127942; . So we&#39;ve seen how the genres relate to each other in terms of percentages per month. We can also see what the top genres are per month, but it can definitely still be improved. I really just want a list with the top 5 genres per month, ideally easily readable and pretty close to the example we had from Last.fm. . As a reminder, that looked like this: . . We can get a list of the top genres per month by grouping and then applying list on the Series. . top_genres_per_month = top_genres_per_month_with_perc.groupby([&#39;year&#39;, &#39;month&#39;]).genre.apply(list).reset_index() top_genres_per_month[:2] . year month genre . 0 2016 | 9 | [rap, pop rap, hip hop, pop, indie pop rap] | . 1 2016 | 10 | [edm, pop, rap, electro house, hip hop] | . We then create a numpy array from these values and apply them column by column to new dataframe columns. . genre_array = np.stack(top_genres_per_month.genre.values) for i, new_col in enumerate([f&#39;genre_{x}&#39; for x in range(1, 6)]): top_genres_per_month[new_col] = genre_array[:, i] top_genres_per_month = top_genres_per_month.drop(&#39;genre&#39;, axis=1) . Until we finally arrive at the following dataframe. On the x-axis we have the top 5 genres, named genre_1 till genre_5, while on the y-axis we have months per year. This is pretty much what I set out to do, so I&#39;m happy with the result. . top_genres_per_month = top_genres_per_month.set_index([&#39;year&#39;, &#39;month&#39;]).T top_genres_per_month . year 2016 2017 ... 2018 2019 . month 9 10 11 12 1 2 3 4 5 6 ... 4 5 6 7 8 9 10 11 12 1 . genre_1 rap | edm | edm | adult standards | pop | electro house | pop rap | rap | rap | pop | ... | rap | edm | rap | rap | rap | rap | rap | rap | rap | rap | . genre_2 pop rap | pop | pop | easy listening | edm | filter house | rap | pop rap | pop rap | edm | ... | hip hop | rap | pop rap | hip hop | edm | hip hop | hip hop | hip hop | hip hop | hip hop | . genre_3 hip hop | rap | adult standards | christmas | rock | dance-punk | edm | hip hop | hip hop | electro house | ... | edm | electro house | hip hop | pop rap | hip hop | pop rap | edm | pop rap | pop rap | pop rap | . genre_4 pop | electro house | christmas | lounge | dance pop | electronic | hip hop | conscious hip hop | pop | brostep | ... | pop | hip hop | edm | edm | pop rap | edm | pop rap | pop | pop | edm | . genre_5 indie pop rap | hip hop | easy listening | dutch hip hop | tropical house | alternative dance | pop | west coast rap | conscious hip hop | electronic trap | ... | pop rap | pop | pop | electro house | electro house | pop | pop | edm | emo rap | electro house | . 5 rows × 29 columns . However, the lack of color makes interpreting this table still fairly challenging. Let&#39;s see if we can improve that a bit. . To style, we can use the style (docs) attribute of pd.DataFrame. This is an easy and super handy way of styling dataframes. It has two main methods: .applymap and .apply. The first one is applied to each cell individually, while the latter is applied to a whole row. That makes .applymap well suited for cell specific layouts, like min-max gradients for example, while .apply works very well for row-based operations, like highlighting the max. . To use them, we need to define a coloring function to apply to the dataframe. As a parameter, we give all the unique values. This allows us to create a mapping, as well as define the number of colors required. The colors we use are RGB colors that aren&#39;t from the standard coloring libraries, like seaborn color palette. This is because none of their palettes support the number of unique values we have, which is 26. So I used the tool called i want hue, that allows the generation of suitable color palettes. Getting 26 unique colors was still not easy (or a great succes in my opinion), but it works at least semi well. . #collapse-hide import seaborn as sns colors_26 = [ &quot;#85cec7&quot;, &quot;#f398d9&quot;, &quot;#afe084&quot;, &quot;#90a9f4&quot;, &quot;#c0c15c&quot;, &quot;#74aff3&quot;, &quot;#e4e88b&quot;, &quot;#d8afec&quot;, &quot;#64ddab&quot;, &quot;#f3a281&quot;, &quot;#52ebd9&quot;, &quot;#ebabbe&quot;, &quot;#9de5a0&quot;, &quot;#a2b8f0&quot;, &quot;#e6bb6d&quot;, &quot;#77cdef&quot;, &quot;#b8c270&quot;, &quot;#b6bee4&quot;, &quot;#9ac68a&quot;, &quot;#4cd1da&quot;, &quot;#dfc299&quot;, &quot;#a0ebe5&quot;, &quot;#c0c38e&quot;, &quot;#8cbca8&quot;, &quot;#d8ebb4&quot;, &quot;#a7e1c1&quot; ] def color_cells(val, unique_values): &quot;&quot;&quot; Takes a cell value and applies coloring depending on the value. Should be applied to a cell, not a row. So use `.applymap`. If value is unknown, defaults to white. &quot;&quot;&quot; # Multiply with 255 to get into css RGB range (0, 255) instead of (0, 1). colors_arr = [tuple(int(y*255) for y in x) for x in sns.color_palette(colors_26)] colormap = [f&#39;rgb{x}&#39; for x in colors_arr] colors = {k: v for k, v in zip(unique_values, colormap)} color = colors.get(val, &#39;white&#39;) return f&#39;background-color: {color}&#39; . . #collapse-hide unique_top_genres = np.unique(top_genres_per_month) # Get a list of unique values for coloring top_genres_per_month.style.applymap(color_cells, unique_values=unique_top_genres) . . year 2016 2017 2018 2019 . month 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 1 . genre_1 rap | edm | edm | adult standards | pop | electro house | pop rap | rap | rap | pop | edm | pop | rap | pop | edm | rap | house | rap | rap | rap | edm | rap | rap | rap | rap | rap | rap | rap | rap | . genre_2 pop rap | pop | pop | easy listening | edm | filter house | rap | pop rap | pop rap | edm | electro house | edm | pop rap | rap | electro house | pop | edm | hip hop | hip hop | hip hop | rap | pop rap | hip hop | edm | hip hop | hip hop | hip hop | hip hop | hip hop | . genre_3 hip hop | rap | adult standards | christmas | rock | dance-punk | edm | hip hop | hip hop | electro house | pop | electro house | hip hop | edm | rap | pop rap | electro house | pop | pop rap | edm | electro house | hip hop | pop rap | hip hop | pop rap | edm | pop rap | pop rap | pop rap | . genre_4 pop | electro house | christmas | lounge | dance pop | electronic | hip hop | conscious hip hop | pop | brostep | brostep | brostep | pop | pop rap | pop rap | hip hop | tech house | pop rap | edm | pop | hip hop | edm | edm | pop rap | edm | pop rap | pop | pop | edm | . genre_5 indie pop rap | hip hop | easy listening | dutch hip hop | tropical house | alternative dance | pop | west coast rap | conscious hip hop | electronic trap | electronic trap | electronic trap | edm | electro house | pop | edm | pop | edm | pop | pop rap | pop | pop | electro house | electro house | pop | pop | edm | emo rap | electro house | . Better get the 🚒 cause this table is 🔥. . This is really close to the Last.fm plot, apart from the lines between points that require 10 years of D3.js experience. We see some similar pattern to those in the earlier plot, but also can see some new insights. Here, we can focus some more on the anomalies that are present, like indie pop rap, dutch hip hop, filter house and conscious hip hop. These stand out more using this representation than before, which focused more on trends. . Insights . More electronic peaks: We can see that February 2017 was actually also a peak in electronic music, but due to similar colors in the previous plot this was a bit hidden. | Pure hip hop periods: Furthermore, we can also see there are some pure hip hop periods, like April and May of 2017, where EDM and electro house are not present at all, and we see more specific hip hop genres make way like west coast rap and conscious hip hop. | . In conclusion . In part 2 we took a closer look at what genres I listen to and how that has developed over time. There were some very interesting insights, like the effects of holidays, and the change of music preference towards rap. We also recreated the plot from Last.fm, as close as possible at least. I&#39;m quite happy with the outcome, but definitely have some newfound respect for Spotify analysts that have to do this for way more people. Although generalization also brings some advantages of course. Doing these analyses also is improving my skills with Pandas, because I have not previously worked that much with time data, so this is a great exercise. Also, having to look into the details of .groupby, and how it operates on timeseries aggregates and what operations are possible were great. For instance, I learned you can do a groupby on a datetime index or column like so: . df.groupby(df[&#39;datetime-column&#39;].dt.year) . and even multi-index this for month/year using: . df.groupby([df[&#39;datetime-column&#39;].dt.year, df[&#39;datetime-column&#39;].dt.month]) . Which is very cool and way cleaner than what I used! But I&#39;m getting sidetracked. . Rounding off; thank you for reading and sticking with me! I&#39;m very curious what results Part 3 will bring. . Topics for part 3: . An analysis of musical features, like energy, danceability and acousticness. Those are numeric values and thus allow for some different visualizations then all of the discrete values of this blogpost. | A look into skipping behaviour -&gt; which songs deserve to be skipped. | Which songs do I listen to that are emo rap. This is probably quite a small point of research, but still I&#39;m quite curious. | .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/spotify-listening-history-analysis-part-2.html",
            "relUrl": "/posts/spotify-listening-history-analysis-part-2.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Analyzing my Spotify listening history 🎵 - Part 1",
            "content": ". Important: I added a Day of Week vs Hour of day plot to visualize weekly behaviour! . Important: Link to Part 2 I like to have everything in my life tracked in some way. Preferably, knowingly (Looking at you, Facebook), cause it allows you to analyze the data and find interesting things (Might be related with becoming a data scientist)! I&#39;ve always been a fan of the features provided by Last.fm to track you listening behaviour across apps and platforms. It allows you to see stuff like your favorite artists per month, or your affinity with certain genres over time like in the image below. . . Buuuuuut, like Last.fm, most of these analyses are paid completely or partly. In the case of Last.fm, you get this plot for free but anything more will cost you some paper. I&#39;m Dutch, so let&#39;s see if we can do it ourselves! . I wanted to have my listening history, and currently there is an API call that provides that functionality. However, I wanted to do this at the start of 2019 (Last year was pretty busy, so I didn&#39;t get around to doing this until now 😅) and this wasn&#39;t available back then, or at least I couldn&#39;t find it. Spotify, like many other companies, has an option to download your personal information. Unfortunately, this data only contained data for three months (they upped it to a year now, which is great!). . But, given this limitation, the only way I could think of to get this was to ask Spotify for my personal data. Under the GDPR, they are required to provide this information, so I thought this had a good shot. Well, after e-mailing back and forth a whole bunch of times, eventually I got in touch with the Data Privacy Office and they provided me with my complete listening history! . So that&#39;s the data that we&#39;ll be working with. Like I said, I requested the data in early 2019, so my history goes from my beginning of Spotify (ca. 2013) until then. So lets see what we&#39;re dealing with. . The Data &#10024; . I received one main file from spotify called EndSong.json which had json items as follows. In total, I got 39,229 songs played, which is quite a lot and definitely enough to do some interesting things with! . { &quot;ts&quot;:&quot;2013-10-09 20:03:57 UTC&quot;, &quot;username&quot;:&quot;xxxxxxxxxx&quot;, &quot;platform&quot;:&quot;xxxxxxx&quot;, &quot;ms_played&quot;:&quot;5969&quot;, &quot;conn_country&quot;:&quot;NL&quot;, &quot;ip_addr_decrypted&quot;:&quot;xx.xx.xx.xx&quot;, &quot;user_agent_decrypted&quot;: &quot;xxxxxxxxxxx&quot;, &quot;master_metadata_track_name&quot;:&quot;You Make Me&quot;, &quot;master_metadata_album_artist_name&quot;:&quot;Avicii&quot;, &quot;master_metadata_album_album_name&quot;:&quot;You Make Me&quot;, &quot;reason_start&quot;:&quot;click-row&quot;, &quot;reason_end&quot;:&quot;click-row&quot;, &quot;shuffle&quot;:false, &quot;skipped&quot;:false, &quot;offline&quot;:false, &quot;offline_timestamp&quot;:&quot;0&quot;, &quot;incognito_mode&quot;:false, &quot;metro_code&quot;:&quot;0&quot;, &quot;longitude&quot;:0, &quot;latitude&quot;:0 } . For our analysis, we&#39;re gonna use the ol&#39; trusty Pandas. The data is in the json-lines format, so we use the python json-lines package to read our data. We&#39;ll also drop some useless columns and convert the timestamp column to a python datetime object. Furthermore, we use the UTF-8 encoding while reading our data, to support tokens that would otherwise be malformed like the ë character. Lastly, we also create separate columns for many of our time attributes like year, month and day, since this makes it easy for filtering during plotting. . Tip: The json-lines format puts a json object on each separate line, and allows for very dense information packaging in json files. Before I knew this, I was reading the data as a string, converting true-&gt;True and false-&gt;False, to match python syntax and then using the ast package to interpret the string as a python object. That also worked ok, but this is much better. 😊 . lines = [] for i in json_lines.reader(open(&#39;data/EndSong.json&#39;, encoding=&#39;utf-8&#39;)): lines.append(i) df = pd.DataFrame(lines) . #collapse-hide df = df.drop([&#39;username&#39;, &#39;user_agent_decrypted&#39;, &#39;incognito_mode&#39;, &#39;platform&#39;, &#39;ip_addr_decrypted&#39;], axis=1) df.ts = pd.to_datetime(df.ts) df[&#39;date&#39;] = df.ts.dt.date df[&#39;year&#39;] = df.ts.dt.year df[&#39;month&#39;] = df.ts.dt.month df[&#39;day&#39;] = df.ts.dt.day df[&#39;dow&#39;] = df.ts.dt.dayofweek df[&#39;hour&#39;] = df.ts.dt.hour df.head(4) . . ts ms_played conn_country master_metadata_track_name master_metadata_album_artist_name master_metadata_album_album_name reason_start reason_end shuffle skipped ... city region episode_name episode_show_name date year month day dow hour . 0 2013-10-09 20:24:30+00:00 | 15010 | NL | Wild for the Night (feat. Skrillex &amp; Birdy Nam... | A$AP Rocky | LONG.LIVE.A$AP (Deluxe Version) | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | 2013-10-09 | 2013 | 10 | 9 | 2 | 20 | . 1 2013-10-09 20:19:20+00:00 | 68139 | NL | Buzzin&#39; | OVERWERK | The Nthº | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | 2013-10-09 | 2013 | 10 | 9 | 2 | 20 | . 2 2013-10-09 20:21:54+00:00 | 23643 | NL | Blue | Gemini | Blue EP | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | 2013-10-09 | 2013 | 10 | 9 | 2 | 20 | . 3 2013-10-09 20:20:29+00:00 | 68063 | NL | Blue | Gemini | Blue EP | unknown | click-row | False | False | ... | NaN | NaN | NaN | NaN | 2013-10-09 | 2013 | 10 | 9 | 2 | 20 | . 4 rows × 25 columns . Yearly &amp; Monthly behaviour &#128198; . One of the first things that might be interesting to see is how my usage of spotify has changed over the years. For this, we can easily plot the number of songs player by year and by month. . Note: All my plots will be created using Altair. This is mainly because fastpages has the best support for this. But, I have taken this opportunity to become really familiar with the nooks and crannies of Altair. My previous go-tos are and likely still will be for most: matplotlib and seaborn. . We&#39;re using the LA Times vega lite theme. We can set altair to use this with the following line. . alt.themes.enable(&#39;latimes&#39;) . ThemeRegistry.enable(&#39;latimes&#39;) . #collapse-hide t = ( df.ts.dt # Use the datetime attributes of the ts column .to_period(&#39;D&#39;) # Convert the ts column to a &#39;date&#39; period. So the timestamp is reduced to a date like &#39;28-2-2020&#39;. .value_counts() # Counts the frequency of all days (i.e. count how many songs were played that day). .to_timestamp() # The index is a RangeIndex after the `to_period`, so we convert it back to a datetime. .to_frame() # Altair wants dataframes, so we convert it to a dataframe. .reset_index() # Make the datetime index a separate column instead of the index. .rename(columns={&#39;index&#39;: &#39;date&#39;, &#39;ts&#39;: &#39;listens&#39;}) # Rename the columns to be descriptive ) yearly = alt.Chart(t).mark_bar(size=30).encode( alt.X(&#39;year(date):O&#39;, title=&#39;Year&#39;), alt.Y(&#39;sum(listens):Q&#39;, title=&#39;Listens&#39;), tooltip=[&#39;sum(listens)&#39;, &#39;year(date)&#39;] ).properties(width=MAXWIDTH/2) monthly = alt.Chart(t).mark_bar().encode( alt.X(&#39;yearmonth(date):O&#39;, title=&#39;Month&#39;), alt.Y(&#39;sum(listens):Q&#39;, title=&#39;Listens&#39;), tooltip=[&#39;sum(listens)&#39;, &#39;yearmonth(date)&#39;], ).properties(width=MAXWIDTH/2) # Altair allows really easy formatting, having horizontal concatenation with the | operator yearly | monthly . . C: Users BaukeBrenninkmeijer Anaconda3 lib site-packages pandas core arrays datetimes.py:1091: UserWarning: Converting to PeriodArray/Index representation will drop timezone information. UserWarning, . In the plots above you see my total songs listened. It immediately becomes clear that I got my data in early 2019, given the drop in 2019 and lack of data in 2020. But what is interesting is the steady increasing line the previous years. It shows that I slowly started using spotify more and more. The start coincides with when I started paying for Spotify as well, which is not very surprising (Yay for no ads and song selection 🤗). . When looking at the number of songs per month, we can still see a decline in listening activity since the spike at October 2018. The peak that ranges from September 2018 to November 2018 can be explained by me starting a new internship where I was playing spotify while working the whole day. In October my total songs listened more than doubled compared to only two months earlier (1615 to 3273 songs played). . Furthermore, we can see that I also used Spotify for a short while in 2016, but stopped using it again for about a year. Then I picked it up again in 2017 and never stopped using it afterwards. This is likely because I was using the web version of Spotify for a while, where you can use adblock to block the ads. But not being able to use it on your phone reduced the utility of Spotify pretty significantly, so I switched back over to my previous way, which was a combination of Poweramp and Google Play Music. . Daily behaviour &#128378; . Well, I&#39;m already learning a lot about Altair, cause creating this plot in its current form easily took three hours. Altair does not like it when you aggregate a value in several places. But the result is also quite a nice visual. I plotted the daily distribution per hours per year. Now, the value is the sum of the whole year, so it&#39;s no wonder that the differences are really similar to what we say in the yearly distribution. More interesting would be the percentage per hour per year, which would tell me something about my listening behaviour throughout the years. . The yearly visuals only show 2016 till 2019, because the others years don&#39;t have enough data. . There are several noteworthy things: . In 2016, there was a big spike between 13:00 and 14:00. 2016 is split between my third and fourth year at university, of which in the fourth year I also was on the board of the e-sports association. I just barely didn&#39;t have enough credits to get my bachelor&#39;s degree in my third year, so my fourth year was pretty empty. The combination likely contributed to many days where I had lunch and then closed myself of with music, to work on association matters. | In 2016, there is a big spike at 9:00. This makes sense, because that was always the time I was cycling to my university. Over the years, I started listening to podcasts more, which is why you can see the 9:00 value decline over 4 years. | In 2018 and 2019, my listening during evening hours decreased quite significantly. Earlier, I had a spike at 22:00 but this completely faded during the first month of 2019. What happened? Not sure, to be frank. It might be that I had more nights planned with friends? | In 2017 and 2018, I somehow play 4% of my music daily before 7:00. Now, this is essentially impossible since I almost never get up before 7. I&#39;m not sure why this is shown to be the case. . Tip: I&#8217;ve since learned that you can so things like define variables and aggregate values in Altair. An example can be found on their website. However, did not go back and redo the analyses using that. That&#8217;s for another time. 😉 | Weekly behaviour . To get a clear view of weekly listening behaviour, we can create a heatmap with the hours and day of week, with the color indicating the number of listens on that hour. We can see that the working week made a lot of difference, with listening mainly focused on Tuesday, Wednesday, Thursday and Friday, with a little bit as well on Monday afternoon. We all need to wake up a bit first on Monday morning ☕. . Interestingly enough, we see some hours that have no plays at all like Monday morning 2:00-3:00. I&#39;m a bit skeptic that I never plays anything at all there, but I don&#39;t have an explanation for it. . Genres &#127911; . Now that we have some insight into my listening behaviour, we can analyze what I listen to a bit more closely. For example, what genres do I listen to most and how do these change? Here we get a bit closer to the visualization we got from Last.fm. . However, before we can do that, we need to get the genres associated with our songs. This was not included in the data Spotify sent over, so we need to use their API to get this information. Spotify does not apply categories per song, but rather per artist. Internally, Spotify uses special URIs to indicate different concepts like artists, songs and albums. For example, a track can be indicated with spotify:track:6rqhFgbbKwnb9MLmUQDhG6. . These URIs refer to a specific object, whereas the artist in the data I received does not. So we need to use the Spotify search function to retrieve the correct Spotify object of an artist, and then we can retrieve the genres from there. Now, either the data they sent is somewhat corrupted or their music management is a bit lackluster, because there were still quite some artists without an Spotify URI and/or without any defined genres. The latter makes sense, since this takes a lot of work by spotify. . You can see the API calls and retrieval loop used in the two collapsed cells below. . #collapse-hide SPOTIFY_API_KEY = os.getenv(&#39;SPOTIFY_API_KEY&#39;) SPOTIFY_CLIENT_ID = os.getenv(&#39;SPOTIFY_CLIENT_ID&#39;) spotify_search_url = &#39;https://api.spotify.com/v1/search?q={q}&amp;type={type}&#39; spotify_headers = {&quot;Accept&quot;: &quot;application/json&quot;, &quot;Authorization&quot;:&quot;Bearer &quot; + SPOTIFY_API_KEY, &quot;Content-Type&quot;: &quot;application/json&quot;} client_credentials_manager = SpotifyClientCredentials(SPOTIFY_CLIENT_ID, SPOTIFY_API_KEY) spotify = spotipy.Spotify(client_credentials_manager=client_credentials_manager) . . artist_ids = [] genres = [] for artist in tqdm(df.master_metadata_album_artist_name): try: response = spotify.search(artist, type=&#39;artist&#39;) artist_id = response.get(&#39;artists&#39;).get(&#39;items&#39;)[0].get(&#39;uri&#39;) artist_genres = response.get(&#39;artists&#39;).get(&#39;items&#39;)[0].get(&#39;genres&#39;) except Exception as e: print(e) print(response) artist_id = &#39;&#39; artist_genres = [] genres.append(artist_genres) artist_ids.append(artist_id) print(len(artist_ids), len(genres)) genres_df = pd.DataFrame(genres) artist_ids_df = pd.DataFrame(artist_ids) . Let&#39;s have a short look at the data. I&#39;m showing the first two rows (so the genres of the first two songs) and all the columns. The columns just have a number, but indicate the first to twenty-first genre of each song. . 0 1 2 3 4 5 6 7 8 9 ... 11 12 13 14 15 16 17 18 19 20 . 0 east coast hip hop | hip hop | pop | pop rap | rap | trap music | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 catstep | complextro | edm | electro house | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 rows × 21 columns . Because we added all genres for each artist as a new column, we have quite a wide dataframe. The artist with the most genres has 21(!) genres. Let&#39;s see who that is. . comb = pd.concat([df, genres_df], axis=1) comb.loc[~(comb[&#39;20&#39;].isna())] . ts ms_played conn_country master_metadata_track_name master_metadata_album_artist_name master_metadata_album_album_name reason_start reason_end shuffle skipped ... 11 12 13 14 15 16 17 18 19 20 . 32948 2018-10-26 21:01:59+00:00 | 255626 | NL | Grounded | Pavement | Crooked Rain Crooked Rain | clickrow | trackdone | False | NaN | ... | indie pop | indie rock | lo-fi | modern rock | new wave | noise pop | noise rock | post-punk | rock | slow core | . 1 rows × 46 columns . It&#39;s some artist called Pavement! I&#39;ve no idea who that is, but still interesting to see. The artist seems to be in the indie rock segment, where there are many subgenres, so it&#39;s not that surprising. I&#39;ve heard that metal has a similar amount of subgenres, so it would be cool to do this analysis for a metal fan 🤘. . But, clearly I am not in that segment. Lets see how many genres an average artist of mine has. We&#39;ll exclude artists with zero genres. . If we take the percentage of the artists with three or less genres, we see this is 52%. This is quite high, and means many people are pretty specific with regards to what genres they fall into for Spotify. We see quite a long tail distribution, with only 27% having more than 5 genres specified and only 2.8% more then 10 genres! . In conclusion . We have done a pretty thorough analysis of my listening history on Spotify. We evaluated the high level listening behaviour on a monthly and yearly basis. We have also seen my daily listening behaviour and how it has changed throughout the years. We also started on the analysis of the genres, which we will continue in part 2! . It has been really interesting to see how my preferences with regards to music over these years, and it is definitely contributing to my &#39;have everything tracked&#39; KPIs. Since all of Spotify&#39;s data is accessible through the API, I might consider making a dashboard for these insights that updates automatically. . . Unfortunately, we didn&#39;t get to see the really cool stuff in this post. Things like recreation of the last.fm image and the changes in genres over time are very interesting, and I would have loved to already be able to show those. Please check out part 2 for that. I&#39;ll add links to that here as soon as that&#39;s out. . Topics covered in part 2: . What are my top genres? | Correlation between genres. | How have my genres changed over time? | Recreation of the Last.fm image. | Learnings . This blogpost has been a huge learning experience for me. It was my first time using Fastpages. It was my first time writing a blogpost in a jupyter notebook as well, and it was also my first time using Altair! All of those experiences were quite positive, and I especially like getting more familiar with Altair. Having a Grammar of Graphics tool in your toolbelt is an extremely valuable thing in the world of data science, although you might not use it on a daily basis. . If you liked this blogpost, don&#39;t hesitate to reach out to me on linkedin or twitter. 😊 . .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/spotify-listening-history-analysis-part-1.html",
            "relUrl": "/posts/spotify-listening-history-analysis-part-1.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Automated product recognition for hospitality industry insights 🍻",
            "content": "We’re all a bit too familiar with membership cards of supermarket chains, like Walmart or Albert Heijn, the largest supermarket chain in the Netherlands. Besides the advantages these cards have for the consumer, they have a lot more advantages for the supermarket chains. They apply advanced data science techniques to the data gathered with these cards to figure out where to put products in the store, what products to put next to each other and what products to put on sale together. . For 6 months I interned at TAPP, a company that tries to bring the same insights to the hospitality industry. Because there are no membership cards for bars (in most cases), we do this by analyzing the products on receipts. Because of the inconsistent offerings at venues, TAPP focuses almost exclusively on drinks, due to the clearly branded single units used. Doing this gives us a very detailed view of the market for drinks, allowing us to see, for example, market shares and revenues for specific sodas, liquors and beers. . Every consumption we receive is connected to a product. Before we can use products in our analysis, a product needs to be ‘tagged’. This tagging means we need to specify what a product is, because the description on a receipt is not enough for insights. A description might be ‘Hot choco’, and to use this for our analysis we need to specialize our five tag levels, which are group, category, subcategory, type and brand. This is also the hierarchical order, so a category has zero or more subcategories which has in turn zero or more types. I purposely omit group because the group tag consists of the values ‘Drinks’, ‘Food’ and ‘Other’, and we only have categories for ‘Drinks’, because of our focus. The hierarchy is visualized in the image below. . . Sounds great, so whats the problem? . Well, a product is identified by the unique combination of the venue, the description and the price. This means a coke in small, medium and large are three different products at a single venue. And this goes for every venue. This means that when connecting with a new venue, we get somewhere between 300 and 2000 new products. These all need to be tagged, which was up until now all done manually. You can image this is a very slow, error-prone process. . And this is where the fun begins. Because this is a very well suited case for some good ol’ machine learning. The goal is to substitute the human labour in the tagging process by a machine learning model. In the image below you can see the data we are working with. Variables that are useful for classification are the description and the unit price. We start of without any of the tags and want to end with them filled in. Because sometimes we just don’t have enough information for some tag, the model also has to be able to predict empty values. This is also visible in the images below, where the brand is left empty because the description ‘koffie’ (coffee) does not give us enough information to fill this. . . . Because the hierarchical structure of our tags holds a lot information, the best approach seemed to classify each tag separately, starting at the top with group and working our way down to brand. This way, lower level tags can use the higher level tag information for their predictions. . There is now way around getting a bit technical, so if that is not your thing you can skip to the results. . Approach . There are two parts to solve this problem. First, we need a model that is capable of reliably predicting these tags. Second, we need to implement this model in our current AWS based infrastructure. . General . Due to our data diversity, this is quite a complex problem. There is natural language processing (NLP) involved in handling the descriptions, as well as our tags. The tags can be regarded as either text or categorical variables (but we’ll see soon this doesn’t matter). The price is fairly simple, and we’ll just take the normalized price as input. Because we have different types of input, I opted to use a model with multiple inputs. I was most comfortable with Keras, and their functional API supports multiple inputs, so I chose this for implementation. Also, because we are tagging each layer separately, there will be a ‘different’ model for each layers. I’m putting different in quotation marks because the model architecture will be the same, but the weights will be different. . When there is NLP involved, two things are generally going to happen. . Tokenize the words (‘Cola’ -&gt; [23]) . | Use word embeddings ([23] -&gt; [0.3, -0.8, 0.7, 0.4, 0.1]) . | The tokenization required some creativity, because the descriptions need to be split on a space, whereas the tags should not (e.g. ‘Mineral Water’ is one tag). So two tokenizers are used. The problem is that both tokenizers use (partially) the same range of number, meaning that ‘drinks’ and ‘Choco’ can have the same token (unique number). This will be talked about more below. . Word embeddings . There are many different approaches to the word embeddings. There are very recent and advanced representations like BERT and ELMo and a little bit older representations like GloVe and Word2Vec. We can use these pretrained weights, but because our vocabulary has a very slim overlap with normal English, this probably does not improve our result much if any. So I decided to train the embedding layer myself, and with almost 150k descriptions, we can get pretty good representations. In Keras, word embeddings are implemented by creating a fixed size random vector, which is then optimized by training. This vector captures no information about context or position, meaning a lot of information is lost. But because we are doing classification, which doesn’t require these things, this is not a big problem. . One thing to consider about these vectors is that some recent implementations of categorical variables are doing the exact same thing, most notably the authors of Fast.ai. A value is converted to a fixed size vector, to give the value a richer representation, which is then optimized by training. Now, Keras doesn’t have this categorical variable specific approach, but we can just use the same embeddings al the sentences. Because in this case, the representations are the same. To visualize the difference, look at the image below. Here, you can clearly see that the word2vec embeddings capture semantic similarities between sentence b and c, whereas the embeddings trained from scratch to not. . . RNN or CNN? . I opted to try two approaches. Because there is NLP involved, using a recurrent neural network (RNN) with LSTM layers seems like a good idea. All the current state of the art language processing is done using recurrent network with LSTM layers. LSTM layers have, simply said, a memory which they can use to remember was words it has seen previously. This gives them the capacity to find word relations that are close together but also further away and makes them very powerful for language processing. . The duplicate token problem I raised earlier really hurts the LSTM performance, because it is very confusing. I solved this by creating a separate input for the tags. So the double tokens still exist, but they are never seen together. I also created a separate input for the price, where no embedding was needed. The result is a network with three inputs, one for the description, one for the parent tags and one for the price. The description and tags both go into a embedding layer and an LSTM layer. The result is the following network. . . Because our texts are very short, I also wanted to try a convolutional neural network (CNN). Whereas LSTMs are very good for finding relations between words further apart, convolutional layers are very good at finding word structures closer together. Combined with pooling layers we can even detect certain structures in sentences. The same goes for the ‘categorical’ values of the tags. CNNs are already very well known from computer vision, where they have been the state-of-the-art for multiple years. . The duplicate token problem is much less of a problem with the convolutional approach, because the context of a word matters more than the word itself. The odds of finding the same structures with the same tokens in both the tags and the descriptions is marginal with a vocabulary of 16000 words for the descriptions and the 1300 tag combinations. So, if this is not a problem, the tags and descriptions can just be concatenated when doing convolutions. This approach also has the capacity to see certain relations between tags and description. The result is a model with only two inputs. . . Implementation . At TAPP we use two services primarily for our data pipeline: AWS and Airflow. Airflow is a great, open source and free tool to manage data pipelines and the ETL process. If you want to know more about Airflow, I recommend this article. . Every part of our infrastructure lives inside a docker **container. Using ECS, we can easily manage our services and it allows us to quickly **scale up and down, depending on our needs. Additionally, moving our infrastructure to different environments is relatively easy, for example a local development environment. . Predicting or training this model are in our system batch operations, which need a lot of compute power for a short time. For this reason, I opted to implement them using AWS Batch. AWS Batch only supports jobs as docker containers, which is nice because we are already working with those. These jobs are ran by an Airflow DAG which schedules the job using the BatchOperator. This model was the first neural network that was implemented which had one big problem: there was no existing infrastructure for using GPUs. Using a GPU on AWS batch requires a couple of things. . An EC2 instance with a GPU. I opted to use a p2.xlarge instance, which is on of the cheapest GPU instances and features an Nvidia Telsa K80. . | This process requires a GPU enabled Amazon Machine Image (AMI), which are the virtual machines Amazon uses for their instances. Now, there are a couple of GPU enabled AMIs around, most notably the Deep Learning AMIs of Amazon itself, which feature a whole range of preinstalled deep learning libraries. Because we are using docker to run our batches, we do not care about the preinstalled deep learning libraries, but rather much more about the installed CUDA and Nvidia Drivers, that allow us to do GPU operations. . | To run GPU operations in docker, one needs to set the docker runtime to ‘nvidia’. To do this by default, we need to edit the AMI and save it as a custom AMI. We can then use this custom AMI for our AWS compute environment. . | Create an AWS Job Queue. . | Create an AWS Compute Environment with the custom AMI, which handles jobs from the job queue. . | After this is all done, we find ourselves a nice docker image which has the required CUDA libraries and Nvidia drivers installed, along with our desired python version (3.6.x). This actually took some time, because the official TensorFlow images are all python 3.5 (or 2.7, but our codebase is in python 3). The images I settled on was Deepo, by the user Ufoym. Using this in its python 3.6 variant with GPU support worked wonderfully, and required a us to only set environment variables and install some additional python packages during building. Requiring little additional software kept the build time and CI/CD pipeline speed to a reasonable level as well. . In this scenario, training the network really needed a GPU. However, the predictions can be done on just a CPU. This is great, because for that we don’t need the custom AMI and separate EC2 instance. We still do the predictions using Batch, but run them on the same machines we already had available. . Model persistence between training and predictions is done using S3. After training, the weights and tokenizers are uploaded to S3, which are then downloaded before doing predictions. . Results . Both approaches to the problem worked fairly well, but it turned out the convolutional approach outperformed the recurrent approach by multiple percents in some tasks. In the table below the results are compared and it is clear that the convolutional approach outperforms the recurrent approach by significant margins in the group, type and brand tasks. The increase in brand recognition is especially impressive, with over 4% higher accuracy and an error reduction of 48.5%. With higher accuracy in every task and lower convergence time, the convolutional approach is clearly the stronger candidate for this task. Due to the short descriptions and semantically categorical values of the tags, the natural language capacities of the LSTM cannot flourish. . Results of both approaches next to each other. The columns indicate the accuracy for that specific task. It’s clear the convolutional approach has higher accuracy with lower convergence time. . Lastly, the data had big effects on the results. During my time at TAPP, the manual tagging continued, some labels were added, some removed, some relationships were changed. Combined with the human error that was present in the manually tagged products, this has a significant effect on the results. The categorization is still not completely finalized around aggregate products with descriptions like ‘open bar’ and combined products, like cocktails or mixers like Jack and Coke. These products are tagged as two separate products, where one has the other as a parent product. Whether the child product’s group is tagged as drinks or others is still a point of discussion. The same goes for product notes, like extra sauce on fries which are also tagged as a separate product, and where the same discussion is present but for whether it should be food *or *other. The (partial) automation of this tagging, paired with removed errors from the dataset should increase the model performance even more, and I think it is very feasible to get to 99% accuracy in some tasks, but the humans need to figure out how to perform this task before the machines can learn from it. . Because these results are not good enough to replace humans, I implemented a way to interact with the model using the old tagging process. Previously, a table extract is made, sent to the taggers, tagged, sent back and then re-uploaded to our data warehouse. The best way to implement the model is between the extraction and sending to the taggers. In the extracted file, there are columns added for each tag with the model’s prediction and its confidence. If the model is very sure (above 0.99 confidence) the prediction is already filled into the column the human taggers are going to fill. If the confidence is lower, the prediction can be regarded as a recommendation for the taggers. The result of this is as follows, where I removed the other tags for simplicity. Because the confidence is higher than 0.99, the prediction is already filled into the tag. Otherwise, tag_Group would be empty . . Future Work . Sadly, I was not able to do everything I wanted. Among these are ideas that only recently occurred to me, when it was too late to do in-depth experiments. Even though I said earlier using pretrained weights would likely not yield much improvements, it should be checked out to confirm my hypotheses. . Additionally, in my convolutional approach I used only one convolution layer. To bridge some of the distance gap that is present using convolutional layers it might be very fruitful to add more convolutional layers with pooling in between. This way, higher order sentence structures or relations between tags and words can become apparent that are currently lost. . Curious? . If you would like to know more about this project, please comment or send me a message on LinkedIn or hit me up on twitter. .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/automated-product-recognition-for-hospitality-industry-insights.html",
            "relUrl": "/posts/automated-product-recognition-for-hospitality-industry-insights.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to personal website",
            "content": "Introduction to personal website . Welcome! . . After fighting with setting up a custom domain, I’ve not officially set out to write about some interesting topics here. The main challenge for me, by length, will be to work towards rounding something of within a limited amount of time. I always get sucked into diving way too deep into a topic, to a point where I’m not knowledgeable enough anymore to understand, and then I lose interest :sweat_smile:. This time will definitely be different! . What do I want to do here? I’d like to write about interesting stuff I find online, discover or think of! Yes, very original :). What does this mean, concretely? . I’m going to talk about: . Deep learnig | Machine learning | Reinforcement Learning | Natural language processing | Computer Vision | Whatever else I find interesting. I can do what I want, mom! | . Current impediments for realizing my dreams: . How do you use latex here? $ lambda$, does this work? | Finding some good topics. | . Thanks and hope to see you! .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/first-personal-post.html",
            "relUrl": "/posts/first-personal-post.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten Tomatoes Rating:Q&#39;, y=&#39;IMDB Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . &lt;VegaLite 3 object&gt; If you see this message, it means the renderer has not been properly enabled for the frontend that you are using. For more information, see https://altair-viz.github.io/user_guide/troubleshooting.html . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten Tomatoes Rating:Q&#39;, y=alt.Y(&#39;IMDB Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release Date:N&#39;, &#39;IMDB Rating:Q&#39;, &#39;Rotten Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . &lt;VegaLite 3 object&gt; If you see this message, it means the renderer has not been properly enabled for the frontend that you are using. For more information, see https://altair-viz.github.io/user_guide/troubleshooting.html . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . &lt;VegaLite 3 object&gt; If you see this message, it means the renderer has not been properly enabled for the frontend that you are using. For more information, see https://altair-viz.github.io/user_guide/troubleshooting.html . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide Gross&#39;, &#39;Production Budget&#39;, &#39;Distributor&#39;, &#39;MPAA Rating&#39;, &#39;IMDB Rating&#39;, &#39;Rotten Tomatoes Rating&#39;]].head() . Title Worldwide Gross Production Budget Distributor MPAA Rating IMDB Rating Rotten Tomatoes Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.baukebrenninkmeijer.nl/posts/test.html",
            "relUrl": "/posts/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Hi there, I’m Bauke 👋 . I’m a deep learning practitioner🧠/data scientist📈. I have a background in computer science and data science, and thus both are very prevelant in my work. I have created some bots, some deep learning implementations and some libraries related to synthetic data. . 🔭 I’m currently working on setting up a personal website and synthetic data using GANs. | 🌱 I’m currently learning Full Stack (Deep/machine) Learning | 💬 Ask me about PyTorch and SOTA NLP or Computer Vision models! 🔥🔥 | 😊 My favorite machine learning sources are HuggingFace, Sotabench and paperswithcode | . Projects . Master Thesis . . In my thesis, I researched improvements that we can make to Generative Adversarial Networks (GANs), to apply them better to tabular data. Contrary to GANs for vision tasks, GANs for tabular data are still very early work with only some researchers working on it. Apart from two improvements to the GAN architecture, I also wrote an open source library that focuses on how to evaluate synthetic data. You can find the github repos and the thesis PDF below. . Repo for my thesis: Star | Repo for evaluation library: Star | Find my thesis on the Radboud University website. | .",
          "url": "https://www.baukebrenninkmeijer.nl/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "CV",
          "content": "Below, you can find my resume in PDF format. . This browser does not support PDFs. Please download the PDF to view it: Download PDF. . &lt;/embed&gt;",
          "url": "https://www.baukebrenninkmeijer.nl/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.baukebrenninkmeijer.nl/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}