{
  "hash": "3f72689f2ebea749c570a93ade1b1b1a",
  "result": {
    "markdown": "---\nauthor: Bauke Brenninkmeijer\nbadges: true\nbranch: master\ncategories:\n- Genetic Algorithms\n- Machine learning\ndate: '2023-02-16'\nimage: images/genetic-algorithms/banner.png\ntitle: Genetic Algorithms for image reconstruction ðŸ§¬\ndescription: How we can leverage genetic algorithms to help with image reconstruction.\ntoc: true\nformat:\n  html:\n    # code-fold: show\n    code-tools: true\n    # highlight-style: github\n    code-line-numbers: true\n    code-overflow: scroll\n    code-block-border-left: true\n---\n\n## Introduction\n\n![Best individual, target and two loss metrics](images/genetic-algorithms/4dots_final_state.png){#fig-4dots-endstate}\n\nGenetic algorithms are a beautiful subset in the machine learning domain, that use evolutionairy techniques to arrive to solutions that other algorithms have trouble at. These techniques include the combining of solutions, often called crossover, and the slightly altering of solutions, called mutations.\n\nIn this post, I'll show two versions genetic algorithms that can be used for image reconstruction given a target image. In the real world, it will have few applications but it's a great theoretical exercise and practice for understanding of the algorithms and image manipulation. However, they are unmistakingly usefull and have been applied in many domains, one being Neural Architecture Search, a method to find the best architecture for a neural network given a specific problem.\n\nWhile other optimization methods, such a gradient descent are incredibly powerful for problems that provide a smooth non-stochastic optimization curve. However, for problems that might have many different good solutions that are not easily findable following a single trajectory in parameter space, genetic algorithms can provide unexpected solutions.\n\nAs in nature, genetic algorithms are based on natural selection and survival of the fittest. That means that each iteration, we select the best candidates for the problem at hand and continue with those. Each individual represents a solutions, and by slightly altering and combining solutions we hope to come to a better solution each iteration. As you can understand, this is a very generic approach that can be applied to all types of problems.\n\n### The algorithm\n\nGenerally, the algorithms for genetic algorithms follows roughly the same outline and is as follows:\n\n```\npop = create initial popluation with n individuals\n\nfor i in n_iterations:\n    1. randomly combine/reproduce individuals\n    2. randomly mutate individuals\n    3. retain fittest individuals\n```\n\n\n### Mutations\nLet's start with the most simple version of changes made to candidates: mutations. In any organism with DNA (or some form of it), we see mutations; slight changes in the genetic code. In organism, it's typically the results of an incorrect copy of DNA code, but in this case we are intentionally applying mutation to create slight variations.\n\nFor human cells, a mutated cell can start to misbehave, which is generally cleaned up by our immune system. However, sometimes they're missed or not easy to clean up and can lead to serious consequences such as cancer. We also know mutations from sci-fi and monster stories, which result in zombies and the like, but that's unfortunately not what we are talking about today.\n\n### Procreation\nProcreation is very important for this algorithm, because it allows the combining of two (or more) individuals into a new individual. Hopefully, this leads to to an individual that has all good qualities of their parents and none of the bad. In the rest of this post, the terms procreation, crossover and combine are used interchangeably for this concept.\n\nAn example with two individuals and their crossover. In this example, and individual is defined by its genes: 4 binary digits. The crossover in this case is just taking the first two digits of the first individual and the latter two of the second individual.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nind1 = [0, 0, 0, 0]\nind2 = [1, 1, 1, 1]\n\ndef crossover(a, b):\n  return a[:2] + b[2:]\n\nprint(f'Result of crossover: {crossover(ind1, ind2)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResult of crossover: [0, 0, 1, 1]\n```\n:::\n:::\n\n\n### Fitness function\nTo assess which individuals are the most fit, you need some metric. Depending on the problem, naturally you can use many different versions. In our case, image reconstruction, we are going to use the mean-squared error of the pixel values $l = \\frac{1}{n} \\sum_{i=1}^{n}(c_i - T)^2$ where $n$ is the size of the population, $c_i$ is candidate $i$ and $T$ the target image. This works nicely with our data and as always, penalizes the largest errors the most.\n\n## Pixelwise\nIn this method, we try to recreate the target image by manipulating individual pixels and comibing whole pixel arrays. The genes of an individual is a pixel array the size of the target image. For some RGB image, this will be a three dimensional, for example (200, 200, 3).\n\nLet's first deine a pixel individual and set its genes to the shape that we want. We also give an option to pass genes, which is handy for the crossover step later. Lastly, we define that probability to mutate `mutate_p` and the delta of a mutation `mutate_d`. The main methods of the individual are already defined here as well.\n\n\n\n::: {.cell .column-page execution_count=3}\n``` {.python .cell-code}\nclass PixelIndividual:\n    \"\"\"Individual with pixel grid for genes.\"\"\"\n\n    def __init__(\n        self,\n        shape: Tuple,\n        genes: np.ndarray | None = None,\n        mutate_d: float = 0.05,\n        mutate_p: float = 0.2\n    ):\n        self.shape = shape\n        self.mutate_d = mutate_d\n        self.mutate_p = mutate_p\n        if genes is None:\n            self.genes = (np.random.rand(*shape)).astype(np.float32)\n        else:\n            self.genes = genes\n\n```\n:::\n\n\n### Crossover\n\nThe second method is our choice of what happens during crossover. How to implement a crossover function is really up to the person working on the problem. Initially I just took the pixelwise mean of both parents, but that seemed to always kind of move towards hovering around 0.5, which is maybe logical, but definitely undesirable. In this version, I chose to randomly take each pixel from either parent. This seems to work fairly well but you can use many other versions so make sure to play around with this a bit yourself. The `filter_arr` creates an array of random zeros and ones. We use it as a filter to decide which value to pick.\n\n::: {.cell .column-page execution_count=4}\n``` {.python .cell-code}\n def crossover(self, other: PixelIndividual) -> PixelIndividual:\n        filter_arr = np.random.randint(low=0, high=2, size=self.genes.shape)\n        output = np.where(filter_arr, self.genes, other.genes)\n        return PixelIndividual(shape=self.shape, genes=output)\n```\n:::\n\n\n### Mutation\nThen, onto the mutation part, which is fairly simple. We just create some noise of a certain magnitude, shift it so half of it is negative and add it to the existing pixel values of said candidate. In the plot below you can see the distribution of a mutation for an individual with 100x100 grid as genes. I have chosen a uniform distribution for this, but again, you can choose others such as normal. However, the domain of the uniform distribution is simple and intuitive. For example, scaling an uniform distribution with 0.2 will have a magnitude of 0.2 as well.  ranging from 0 to 0.2 distributed evenly. So you can see why working with this distribution is nice.\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](2023-02-15-genetic-algorithms_files/figure-html/cell-6-output-1.png){width=593 height=449}\n:::\n:::\n\n\nAs a last step, we apply the mutation and clip the values to the range [0, 1]. This is because otherwise we can mutate outside of the colour boundaries of an image, which will be clipped when shown as an image anyway. The domain for pixel values with a float is [0, 1] or [0, 255] for integer values, and plotting libraries like matplotlib will clip values for you if you don't. To prevent hidden problems, we already make sure the domains and datatypes are correct.\n\n::: {.cell .column-page execution_count=6}\n``` {.python .cell-code}\n    def mutate(self):\n        if self.mutate_p > np.random.rand():\n            self.mutation = (np.random.rand(*self.shape) * self.mutate_d) - (self.mutate_d * 0.5)\n            self.mutation = self.mutation.astype(np.float32)\n            self.genes = np.clip(self.genes + self.mutation, a_min=0.0, a_max=1.0)\n```\n:::\n\n\nBecause I encountered a lot of datatype issues, such as float64 and integer reprentations, I cast most computation to float32 and also do a dtype check in the `compute_fitness` method. This is because this is the last step of each iteration, and generally should represent if things went correctly.\n\nThe fitness method is in this case the mean-squared error.\n\n::: {.cell .column-page execution_count=7}\n``` {.python .cell-code}\n    def compute_fitness(self, target: np.ndarray):\n        assert self.genes.dtype.name == 'float32', (\n            f'genes dtype should be float32 but found {self.genes.dtype.name}.'\n        )\n        self.fitness = ((self.genes - target) ** 2).mean()\n```\n:::\n\n\n### Result\n\nSome of the results are pretty cool. With smaller images, it works quite fast but for larger it can take several hours to several days to get any good results. The mario image took around 12 hours to create, and while you can clearly see the outlines, it's far from perfect.\n\n#### Toy example: 10x10 matrix\n\nThis run is for just 200 iterations and takes around a minute. The right most graph shows the fitness distribution of the current population. The fitness function here was sum of squared errors, rather than mean.\n\n![Best individual, target and two loss metrics of a 10x10 pixel grid with 4 dots](images/genetic-algorithms/4dots_training.gif){#fig-4dots-training .column-screen}\n\n\n#### Slightly less toy example: 100x100 matrix with gradient\n\nThis run is for 10k iterations and takes around a 1-2 hours. The right most graph shows the fitness distribution of the current population. You can see the graph struggle to show a clear distribution. This seems to mean that the whole distribution is very close, although the values on the x-axis are pretty big, so I'm not entirely sure why it cannot show a good distribution.\n\nYou can see an immediate steep decline in loss, which is attributed to the initial high variety in individuals. The further the iterations go, the more we have selected the optimal individuals and the more of the population can be considered brothers and sisters of the original best individual. When the problem is complex and high-dimensional, this happens more and more, since it's quite unlikely that other candidates can present a better solution from mutation within the timeframe of the best candidate taking over the population.\n\n![Best individual, target and two loss metrics of a 100x100 pixel grid with gradient](images/genetic-algorithms/gradient_training_optimized_compressed.gif){#fig-gradient-training .column-screen}\n\n",
    "supporting": [
      "2023-02-15-genetic-algorithms_files"
    ],
    "filters": [],
    "includes": {}
  }
}