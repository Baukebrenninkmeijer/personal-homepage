[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Below, you can find my resume in PDF format.\n\n\n\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF."
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "",
    "text": "We‚Äôre all a bit too familiar with membership cards of supermarket chains, like Walmart or Albert Heijn, the largest supermarket chain in the Netherlands. Besides the advantages these cards have for the consumer, they have a lot more advantages for the supermarket chains. They apply advanced data science techniques to the data gathered with these cards to figure out where to put products in the store, what products to put next to each other and what products to put on sale together.\nFor 6 months I interned at TAPP, a company that tries to bring the same insights to the hospitality industry. Because there are no membership cards for bars (in most cases), we do this by analyzing the products on receipts. Because of the inconsistent offerings at venues, TAPP focuses almost exclusively on drinks, due to the clearly branded single units used. Doing this gives us a very detailed view of the market for drinks, allowing us to see, for example, market shares and revenues for specific sodas, liquors and beers.\nEvery consumption we receive is connected to a product. Before we can use products in our analysis, a product needs to be ‚Äòtagged‚Äô. This tagging means we need to specify what a product is, because the description on a receipt is not enough for insights. A description might be ‚ÄòHot choco‚Äô, and to use this for our analysis we need to specialize our five tag levels, which are group, category, subcategory, type and brand. This is also the hierarchical order, so a category has zero or more subcategories which has in turn zero or more types. I purposely omit group because the group tag consists of the values ‚ÄòDrinks‚Äô, ‚ÄòFood‚Äô and ‚ÄòOther‚Äô, and we only have categories for ‚ÄòDrinks‚Äô, because of our focus. The hierarchy is visualized in the image below."
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#sounds-great-so-whats-the-problem",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#sounds-great-so-whats-the-problem",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "Sounds great, so whats the problem?",
    "text": "Sounds great, so whats the problem?\nWell, a product is identified by the unique combination of the venue, the description and the price. This means a coke in small, medium and large are three different products at a single venue. And this goes for every venue. This means that when connecting with a new venue, we get somewhere between 300 and 2000 new products. These all need to be tagged, which was up until now all done manually. You can image this is a very slow, error-prone process.\nAnd this is where the fun begins. Because this is a very well suited case for some good ol‚Äô machine learning. The goal is to substitute the human labour in the tagging process by a machine learning model. In the image below you can see the data we are working with. Variables that are useful for classification are the description and the unit price. We start of without any of the tags and want to end with them filled in. Because sometimes we just don‚Äôt have enough information for some tag, the model also has to be able to predict empty values. This is also visible in the images below, where the brand is left empty because the description ‚Äòkoffie‚Äô (coffee) does not give us enough information to fill this.\n\n\n\nSource data for a product\n\n\n\n\n\nTarget data for a product\n\n\nBecause the hierarchical structure of our tags holds a lot information, the best approach seemed to classify each tag separately, starting at the top with group and working our way down to brand. This way, lower level tags can use the higher level tag information for their predictions.\nThere is now way around getting a bit technical, so if that is not your thing you can skip to the results."
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#approach",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#approach",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "Approach",
    "text": "Approach\nThere are two parts to solve this problem. First, we need a model that is capable of reliably predicting these tags. Second, we need to implement this model in our current AWS based infrastructure.\n\nGeneral\nDue to our data diversity, this is quite a complex problem. There is natural language processing (NLP) involved in handling the descriptions, as well as our tags. The tags can be regarded as either text or categorical variables (but we‚Äôll see soon this doesn‚Äôt matter). The price is fairly simple, and we‚Äôll just take the normalized price as input. Because we have different types of input, I opted to use a model with multiple inputs. I was most comfortable with Keras, and their functional API supports multiple inputs, so I chose this for implementation. Also, because we are tagging each layer separately, there will be a ‚Äòdifferent‚Äô model for each layers. I‚Äôm putting different in quotation marks because the model architecture will be the same, but the weights will be different.\nWhen there is NLP involved, two things are generally going to happen.\n\nTokenize the words (‚ÄòCola‚Äô -&gt; [23])\nUse word embeddings ([23] -&gt; [0.3, -0.8, 0.7, 0.4, 0.1])\n\nThe tokenization required some creativity, because the descriptions need to be split on a space, whereas the tags should not (e.g.¬†‚ÄòMineral Water‚Äô is one tag). So two tokenizers are used. The problem is that both tokenizers use (partially) the same range of number, meaning that ‚Äòdrinks‚Äô and ‚ÄòChoco‚Äô can have the same token (unique number). This will be talked about more below.\n\n\nWord embeddings\nThere are many different approaches to the word embeddings. There are very recent and advanced representations like BERT and ELMo and a little bit older representations like GloVe and Word2Vec. We can use these pretrained weights, but because our vocabulary has a very slim overlap with normal English, this probably does not improve our result much if any. So I decided to train the embedding layer myself, and with almost 150k descriptions, we can get pretty good representations. In Keras, word embeddings are implemented by creating a fixed size random vector, which is then optimized by training. This vector captures no information about context or position, meaning a lot of information is lost. But because we are doing classification, which doesn‚Äôt require these things, this is not a big problem.\nOne thing to consider about these vectors is that some recent implementations of categorical variables are doing the exact same thing, most notably the authors of Fast.ai. A value is converted to a fixed size vector, to give the value a richer representation, which is then optimized by training. Now, Keras doesn‚Äôt have this categorical variable specific approach, but we can just use the same embeddings al the sentences. Because in this case, the representations are the same. To visualize the difference, look at the image below. Here, you can clearly see that the word2vec embeddings capture semantic similarities between sentence b and c, whereas the embeddings trained from scratch to not.\n\n\n\nDifferences between word2vec and trained from scratch word embeddings. Source: L√≥pez-S√°nchez et al.¬†2018\n\n\n\n\nRNN or CNN?\nI opted to try two approaches. Because there is NLP involved, using a recurrent neural network (RNN) with LSTM layers seems like a good idea. All the current state of the art language processing is done using recurrent network with LSTM layers. LSTM layers have, simply said, a memory which they can use to remember was words it has seen previously. This gives them the capacity to find word relations that are close together but also further away and makes them very powerful for language processing.\nThe duplicate token problem I raised earlier really hurts the LSTM performance, because it is very confusing. I solved this by creating a separate input for the tags. So the double tokens still exist, but they are never seen together. I also created a separate input for the price, where no embedding was needed. The result is a network with three inputs, one for the description, one for the parent tags and one for the price. The description and tags both go into a embedding layer and an LSTM layer. The result is the following network.\n\nBecause our texts are very short, I also wanted to try a convolutional neural network (CNN). Whereas LSTMs are very good for finding relations between words further apart, convolutional layers are very good at finding word structures closer together. Combined with pooling layers we can even detect certain structures in sentences. The same goes for the ‚Äòcategorical‚Äô values of the tags. CNNs are already very well known from computer vision, where they have been the state-of-the-art for multiple years.\nThe duplicate token problem is much less of a problem with the convolutional approach, because the context of a word matters more than the word itself. The odds of finding the same structures with the same tokens in both the tags and the descriptions is marginal with a vocabulary of 16000 words for the descriptions and the 1300 tag combinations. So, if this is not a problem, the tags and descriptions can just be concatenated when doing convolutions. This approach also has the capacity to see certain relations between tags and description. The result is a model with only two inputs."
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#implementation",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#implementation",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "Implementation",
    "text": "Implementation\nAt TAPP we use two services primarily for our data pipeline: AWS and Airflow. Airflow is a great, open source and free tool to manage data pipelines and the ETL process. If you want to know more about Airflow, I recommend this article.\nEvery part of our infrastructure lives inside a docker container. Using ECS, we can easily manage our services and it allows us to quickly scale up and down, depending on our needs. Additionally, moving our infrastructure to different environments is relatively easy, for example a local development environment.\nPredicting or training this model are in our system batch operations, which need a lot of compute power for a short time. For this reason, I opted to implement them using AWS Batch. AWS Batch only supports jobs as docker containers, which is nice because we are already working with those. These jobs are ran by an Airflow DAG which schedules the job using the BatchOperator. This model was the first neural network that was implemented which had one big problem: there was no existing infrastructure for using GPUs. Using a GPU on AWS batch requires a couple of things.\n\nAn EC2 instance with a GPU. I opted to use a p2.xlarge instance, which is on of the cheapest GPU instances and features an Nvidia Telsa K80.\nThis process requires a GPU enabled Amazon Machine Image (AMI), which are the virtual machines Amazon uses for their instances. Now, there are a couple of GPU enabled AMIs around, most notably the Deep Learning AMIs of Amazon itself, which feature a whole range of preinstalled deep learning libraries. Because we are using docker to run our batches, we do not care about the preinstalled deep learning libraries, but rather much more about the installed CUDA and Nvidia Drivers, that allow us to do GPU operations.\nTo run GPU operations in docker, one needs to set the docker runtime to ‚Äònvidia‚Äô. To do this by default, we need to edit the AMI and save it as a custom AMI. We can then use this custom AMI for our AWS compute environment.\nCreate an AWS Job Queue.\nCreate an AWS Compute Environment with the custom AMI, which handles jobs from the job queue.\n\nAfter this is all done, we find ourselves a nice docker image which has the required CUDA libraries and Nvidia drivers installed, along with our desired python version (3.6.x). This actually took some time, because the official TensorFlow images are all python 3.5 (or 2.7, but our codebase is in python 3). The images I settled on was Deepo, by the user Ufoym. Using this in its python 3.6 variant with GPU support worked wonderfully, and required a us to only set environment variables and install some additional python packages during building. Requiring little additional software kept the build time and CI/CD pipeline speed to a reasonable level as well.\nIn this scenario, training the network really needed a GPU. However, the predictions can be done on just a CPU. This is great, because for that we don‚Äôt need the custom AMI and separate EC2 instance. We still do the predictions using Batch, but run them on the same machines we already had available.\nModel persistence between training and predictions is done using S3. After training, the weights and tokenizers are uploaded to S3, which are then downloaded before doing predictions."
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#results",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#results",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "Results",
    "text": "Results\nBoth approaches to the problem worked fairly well, but it turned out the convolutional approach outperformed the recurrent approach by multiple percents in some tasks. In the table below the results are compared and it is clear that the convolutional approach outperforms the recurrent approach by significant margins in the group, type and brand tasks. The increase in brand recognition is especially impressive, with over 4% higher accuracy and an error reduction of 48.5%. With higher accuracy in every task and lower convergence time, the convolutional approach is clearly the stronger candidate for this task. Due to the short descriptions and semantically categorical values of the tags, the natural language capacities of the LSTM cannot flourish.\nResults of both approaches next to each other. The columns indicate the accuracy for that specific task. It‚Äôs clear the convolutional approach has higher accuracy with lower convergence time.\nLastly, the data had big effects on the results. During my time at TAPP, the manual tagging continued, some labels were added, some removed, some relationships were changed. Combined with the human error that was present in the manually tagged products, this has a significant effect on the results. The categorization is still not completely finalized around aggregate products with descriptions like ‚Äòopen bar‚Äô and combined products, like cocktails or mixers like Jack and Coke. These products are tagged as two separate products, where one has the other as a parent product. Whether the child product‚Äôs group is tagged as drinks or others is still a point of discussion. The same goes for product notes, like extra sauce on fries which are also tagged as a separate product, and where the same discussion is present but for whether it should be food or other. The (partial) automation of this tagging, paired with removed errors from the dataset should increase the model performance even more, and I think it is very feasible to get to 99% accuracy in some tasks, but the humans need to figure out how to perform this task before the machines can learn from it.\nBecause these results are not good enough to replace humans, I implemented a way to interact with the model using the old tagging process. Previously, a table extract is made, sent to the taggers, tagged, sent back and then re-uploaded to our data warehouse. The best way to implement the model is between the extraction and sending to the taggers. In the extracted file, there are columns added for each tag with the model‚Äôs prediction and its confidence. If the model is very sure (above 0.99 confidence) the prediction is already filled into the column the human taggers are going to fill. If the confidence is lower, the prediction can be regarded as a recommendation for the taggers. The result of this is as follows, where I removed the other tags for simplicity. Because the confidence is higher than 0.99, the prediction is already filled into the tag. Otherwise, tag_Group would be empty"
  },
  {
    "objectID": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#future-work",
    "href": "posts/2020-07-24-automated-product-recognition-for-hospitality-industry-insights.html#future-work",
    "title": "Automated product recognition for hospitality industry insights üç∫",
    "section": "Future Work",
    "text": "Future Work\nSadly, I was not able to do everything I wanted. Among these are ideas that only recently occurred to me, when it was too late to do in-depth experiments. Even though I said earlier using pretrained weights would likely not yield much improvements, it should be checked out to confirm my hypotheses.\nAdditionally, in my convolutional approach I used only one convolution layer. To bridge some of the distance gap that is present using convolutional layers it might be very fruitful to add more convolutional layers with pooling in between. This way, higher order sentence structures or relations between tags and words can become apparent that are currently lost.\n\nCurious?\nIf you would like to know more about this project, please comment or send me a message on LinkedIn or hit me up on twitter."
  },
  {
    "objectID": "posts/2020-07-31-spotify-listening-history-analysis-part-1.html",
    "href": "posts/2020-07-31-spotify-listening-history-analysis-part-1.html",
    "title": "Analyzing my Spotify listening history üéµ - Part 1",
    "section": "",
    "text": "Important\n\n\n\nI added a Day of Week vs Hour of day plot to visualize weekly behaviour!\nI like to have everything in my life tracked in some way. Preferably, knowingly (Looking at you, Facebook), cause it allows you to analyze the data and find interesting things (Might be related with becoming a data scientist)! I‚Äôve always been a fan of the features provided by Last.fm to track you listening behaviour across apps and platforms. It allows you to see stuff like your favorite artists per month, or your affinity with certain genres over time like in the image below.\nBuuuuuut, like Last.fm, most of these analyses are paid completely or partly. In the case of Last.fm, you get this plot for free but anything more will cost you some paper. I‚Äôm Dutch, so let‚Äôs see if we can do it ourselves!\nI wanted to have my listening history, and currently there is an API call that provides that functionality. However, I wanted to do this at the start of 2019 (Last year was pretty busy, so I didn‚Äôt get around to doing this until now üòÖ) and this wasn‚Äôt available back then, or at least I couldn‚Äôt find it. Spotify, like many other companies, has an option to download your personal information. Unfortunately, this data only contained data for three months (they upped it to a year now, which is great!).\nBut, given this limitation, the only way I could think of to get this was to ask Spotify for my personal data. Under the GDPR, they are required to provide this information, so I thought this had a good shot. Well, after e-mailing back and forth a whole bunch of times, eventually I got in touch with the Data Privacy Office and they provided me with my complete listening history!\nSo that‚Äôs the data that we‚Äôll be working with. Like I said, I requested the data in early 2019, so my history goes from my beginning of Spotify (ca. 2013) until then. So lets see what we‚Äôre dealing with."
  },
  {
    "objectID": "posts/2020-07-31-spotify-listening-history-analysis-part-1.html#the-data",
    "href": "posts/2020-07-31-spotify-listening-history-analysis-part-1.html#the-data",
    "title": "Analyzing my Spotify listening history üéµ - Part 1",
    "section": "The Data ‚ú®",
    "text": "The Data ‚ú®\nI received one main file from spotify called EndSong.json which had json items as follows. In total, I got 39,229 songs played, which is quite a lot and definitely enough to do some interesting things with!\n{\n    \"ts\":\"2013-10-09 20:03:57 UTC\",\n    \"username\":\"xxxxxxxxxx\",\n    \"platform\":\"xxxxxxx\",\n    \"ms_played\":\"5969\",\n    \"conn_country\":\"NL\",\n    \"ip_addr_decrypted\":\"xx.xx.xx.xx\",\n    \"user_agent_decrypted\": \"xxxxxxxxxxx\",\n    \"master_metadata_track_name\":\"You Make Me\",\n    \"master_metadata_album_artist_name\":\"Avicii\",\n    \"master_metadata_album_album_name\":\"You Make Me\",\n    \"reason_start\":\"click-row\",\n    \"reason_end\":\"click-row\",\n    \"shuffle\":false,\n    \"skipped\":false,\n    \"offline\":false,\n    \"offline_timestamp\":\"0\",\n    \"incognito_mode\":false,\n    \"metro_code\":\"0\",\n    \"longitude\":0,\n    \"latitude\":0\n}\nFor our analysis, we‚Äôre gonna use the ol‚Äô trusty Pandas. The data is in the json-lines format, so we use the python json-lines package to read our data. We‚Äôll also drop some useless columns and convert the timestamp column to a python datetime object. Furthermore, we use the UTF-8 encoding while reading our data, to support tokens that would otherwise be malformed like the √´ character. Lastly, we also create separate columns for many of our time attributes like year, month and day, since this makes it easy for filtering during plotting.\n\n\n\n\n\n\nTip\n\n\n\nThe json-lines format puts a json object on each separate line, and allows for very dense information packaging in json files. Before I knew this, I was reading the data as a string, converting true-&gt;True and false-&gt;False, to match python syntax and then using the ast package to interpret the string as a python object. That also worked ok, but this is much better. üòä\n\n\n\nlines = []\nfor i in json_lines.reader(open('data/EndSong.json', encoding='utf-8')):\n    lines.append(i)\ndf = pd.DataFrame(lines)\n\n\n\nCode\ndf = df.drop(['username', 'user_agent_decrypted', 'incognito_mode', 'platform', 'ip_addr_decrypted'], axis=1)\ndf.ts = pd.to_datetime(df.ts)\ndf['date'] = df.ts.dt.date\ndf['year'] = df.ts.dt.year\ndf['month'] = df.ts.dt.month\ndf['day'] = df.ts.dt.day\ndf['dow'] = df.ts.dt.dayofweek\ndf['hour'] = df.ts.dt.hour\ndf.head(4)\n\n\n\n\n\n\n\n\n\nts\nms_played\nconn_country\nmaster_metadata_track_name\nmaster_metadata_album_artist_name\nmaster_metadata_album_album_name\nreason_start\nreason_end\nshuffle\nskipped\n...\ncity\nregion\nepisode_name\nepisode_show_name\ndate\nyear\nmonth\nday\ndow\nhour\n\n\n\n\n0\n2013-10-09 20:24:30+00:00\n15010\nNL\nWild for the Night (feat. Skrillex & Birdy Nam...\nA$AP Rocky\nLONG.LIVE.A$AP (Deluxe Version)\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\n2013-10-09\n2013\n10\n9\n2\n20\n\n\n1\n2013-10-09 20:19:20+00:00\n68139\nNL\nBuzzin'\nOVERWERK\nThe Nth¬∫\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\n2013-10-09\n2013\n10\n9\n2\n20\n\n\n2\n2013-10-09 20:21:54+00:00\n23643\nNL\nBlue\nGemini\nBlue EP\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\n2013-10-09\n2013\n10\n9\n2\n20\n\n\n3\n2013-10-09 20:20:29+00:00\n68063\nNL\nBlue\nGemini\nBlue EP\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\n2013-10-09\n2013\n10\n9\n2\n20\n\n\n\n\n4 rows √ó 25 columns"
  },
  {
    "objectID": "posts/2023-02-15-genetic-algorithms.html",
    "href": "posts/2023-02-15-genetic-algorithms.html",
    "title": "Genetic Algorithms for image reconstruction üß¨",
    "section": "",
    "text": "Tip\n\n\n\nAll code for this project can be found my github page"
  },
  {
    "objectID": "posts/2023-02-15-genetic-algorithms.html#introduction",
    "href": "posts/2023-02-15-genetic-algorithms.html#introduction",
    "title": "Genetic Algorithms for image reconstruction üß¨",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nFigure¬†1: Best individual, target and two loss metrics\n\n\n\nGenetic algorithms are a beautiful subset in the machine learning domain, that use evolutionairy techniques to arrive to solutions that other algorithms have trouble at. These techniques include the combining of solutions, often called crossover, and the slightly altering of solutions, called mutations.\nIn this post, I‚Äôll show two versions genetic algorithms that can be used for image reconstruction given a target image. In the real world, it will have few applications but it‚Äôs a great theoretical exercise and practice for understanding of the algorithms and image manipulation. However, they are unmistakingly usefull and have been applied in many domains, one being Neural Architecture Search, a method to find the best architecture for a neural network given a specific problem.\nWhile other optimization methods, such a gradient descent, are incredibly powerful for problems that provide a smooth non-stochastic optimization curve, they frequently lack the variance needed for problems with solutions not easily findable following a single trajectory in parameter space. This is where genetic algorithms can provide unexpected solutions.\nAs in nature, genetic algorithms are based on natural selection and survival of the fittest. That means that each iteration, we select the best candidates for the problem at hand and continue with those. Each individual represents a solutions, and by slightly altering and combining solutions we hope to come to a better solution each iteration. As you can understand, this is a very generic approach that can be applied to all types of problems.\n\nThe algorithm\nGenerally, the algorithms for genetic algorithms follows roughly the same outline and is as follows:\npop = create initial popluation with n individuals\n\nfor i in n_iterations:\n    1. randomly combine/reproduce individuals\n    2. randomly mutate individuals\n    3. retain fittest individuals\n\n\nMutations\nLet‚Äôs start with the most simple version of changes made to candidates: mutations. In any organism with DNA (or some form of it), we see mutations; slight changes in the genetic code. In organism, it‚Äôs typically the results of an incorrect copy of DNA code, but in this case we are intentionally applying mutation to create slight variations.\nFor human cells, a mutated cell can start to misbehave, which is generally cleaned up by our immune system. However, sometimes they‚Äôre missed or not easy to clean up and can lead to serious consequences such as cancer. We also know mutations from sci-fi and monster stories, which result in zombies and the like, but that‚Äôs unfortunately not what we are talking about today.\n\n\nProcreation\nProcreation is very important for this algorithm, because it allows the combining of two (or more) individuals into a new individual. Hopefully, this leads to to an individual that has all good qualities of their parents and none of the bad. In the rest of this post, the terms procreation, crossover and combine are used interchangeably for this concept.\nAn example with two individuals and their crossover. In this example, and individual is defined by its genes: 4 binary digits. The crossover in this case is just taking the first two digits of the first individual and the latter two of the second individual.\n\nind1 = [0, 0, 0, 0]\nind2 = [1, 1, 1, 1]\n\ndef crossover(a, b):\n  return a[:2] + b[2:]\n\nprint(f'Result of crossover: {crossover(ind1, ind2)}')\n\nResult of crossover: [0, 0, 1, 1]\n\n\n\n\nFitness function\nTo assess which individuals are the most fit, you need some metric. Depending on the problem, naturally you can use many different versions. In our case, image reconstruction, we are going to use the mean-squared error of the pixel values \\(l = \\frac{1}{n} \\sum_{i=1}^{n}(c_i - T)^2\\) where \\(n\\) is the size of the population, \\(c_i\\) is candidate \\(i\\) and \\(T\\) the target image. This works nicely with our data and as always, penalizes the largest errors the most."
  },
  {
    "objectID": "posts/2023-02-15-genetic-algorithms.html#pixelwise",
    "href": "posts/2023-02-15-genetic-algorithms.html#pixelwise",
    "title": "Genetic Algorithms for image reconstruction üß¨",
    "section": "Pixelwise",
    "text": "Pixelwise\nIn this method, we try to recreate the target image by manipulating individual pixels and comibing whole pixel arrays. The genes of an individual is a pixel array the size of the target image. For some RGB image, this will be a three dimensional, for example (200, 200, 3).\nLet‚Äôs first deine a pixel individual and set its genes to the shape that we want. We also give an option to pass genes, which is handy for the crossover step later. Lastly, we define that probability to mutate mutate_p and the delta of a mutation mutate_d. The main methods of the individual are already defined here as well.\n\nclass PixelIndividual:\n    \"\"\"Individual with pixel grid for genes.\"\"\"\n\n    def __init__(\n        self,\n        shape: Tuple,\n        genes: np.ndarray | None = None,\n        mutate_d: float = 0.05,\n        mutate_p: float = 0.2\n    ):\n        self.shape = shape\n        self.mutate_d = mutate_d\n        self.mutate_p = mutate_p\n        self.genes = np.random.rand(*shape).astype(np.float32) if not genes else genes\n\n\nCrossover\nThe second method is our choice of what happens during crossover. How to implement a crossover function is really up to the person working on the problem. Initially I just took the pixelwise mean of both parents, but that seemed to always kind of move towards hovering around 0.5, which is maybe logical, but definitely undesirable. In this version, I chose to randomly take each pixel from either parent. This seems to work fairly well but you can use many other versions so make sure to play around with this a bit yourself. The filter_arr creates an array of random zeros and ones. We use it as a filter to decide which value to pick.\n\n def crossover(self, other: PixelIndividual) -&gt; PixelIndividual:\n        filter_arr = np.random.randint(low=0, high=2, size=self.genes.shape)\n        output = np.where(filter_arr, self.genes, other.genes)\n        return PixelIndividual(shape=self.shape, genes=output)\n\n\n\nMutation\nThen, onto the mutation part. We create some noise of a certain magnitude, shift it so the mean is zero and half of it is negative and add it to the existing pixel values of said candidate. In the plot below you can see the distribution of a mutation for an individual with 100x100 grid as genes. I have chosen a uniform distribution for this, but again, you can choose others such as normal. However, the domain of the uniform distribution is simple and intuitive. For example, scaling an uniform distribution with 0.2 will have a magnitude of 0.2 as well, ranging from 0 to 0.2 distributed evenly. So you can see why working with this distribution is nice.\n\n\n\n\n\n\n\n\n\nAs a last step, we apply the mutation and clip the values to the range [0, 1]. This is because otherwise we can mutate outside of the colour boundaries of an image, which will be clipped when shown as an image anyway. The domain for pixel values with a float is [0, 1] or [0, 255] for integer values, and plotting libraries like matplotlib will clip values for you if you don‚Äôt. To prevent hidden problems, we already make sure the domains and datatypes are correct.\n\n    def mutate(self):\n        if self.mutate_p &gt; np.random.rand():\n            self.mutation = (np.random.rand(*self.shape) * self.mutate_d) - (self.mutate_d * 0.5)\n            self.mutation = self.mutation.astype(np.float32)\n            self.genes = np.clip(self.genes + self.mutation, a_min=0.0, a_max=1.0)\n\nBecause I encountered a lot of datatype issues, such as float64 and integer reprentations, I cast most computation to float32 and also do a dtype check in the compute_fitness method. This is because this is the last step of each iteration, and generally should represent if things went correctly.\nThe fitness method is in this case the mean-squared error.\n\n    def compute_fitness(self, target: np.ndarray):\n        assert self.genes.dtype.name == 'float32', (\n            f'genes dtype should be float32 but found {self.genes.dtype.name}.'\n        )\n        self.fitness = ((self.genes - target) ** 2).mean()\n\n\n\nResults\nSome of the results are pretty cool. With smaller images, it works quite fast but for larger it can take several hours to several days to get any good results. The mario image took around 12 hours to create, and while you can clearly see the outlines, it‚Äôs far from perfect.\n\nToy example: 10x10 matrix\nThis run is for just 200 iterations and takes around a minute. The right most graph shows the fitness distribution of the current population. The fitness function here was sum of squared errors, rather than mean.\n\n\n\n\n\n\nFigure¬†2: Best individual, target and two loss metrics of a 10x10 pixel grid with 4 dots\n\n\n\n\n\nSlightly less toy example: 100x100 matrix with gradient\nThis run is for 10k iterations and takes around a 1-2 hours. The right most graph shows the fitness distribution of the current population. You can see the graph struggle to show a clear distribution. This seems to mean that the whole distribution is very close, although the values on the x-axis are pretty big, so I‚Äôm not entirely sure why it cannot show a good distribution.\nYou can see an immediate steep decline in loss, which is attributed to the initial high variety in individuals. The further the iterations go, the more we have selected the optimal individuals and the more of the population can be considered brothers and sisters of the original best individual. When the problem is complex and high-dimensional, this happens more and more, since it‚Äôs quite unlikely that other candidates can present a better solution from mutation within the timeframe of the best candidate taking over the population.\n\n\n\n\n\n\nFigure¬†3: Best individual, target and two loss metrics of a 100x100 pixel grid with gradient\n\n\n\n\n\nThe real deal\n\n\n\n\n\n\nFigure¬†4: Best individual, target and two loss metrics of Mario"
  },
  {
    "objectID": "posts/2023-02-15-genetic-algorithms.html#polygons",
    "href": "posts/2023-02-15-genetic-algorithms.html#polygons",
    "title": "Genetic Algorithms for image reconstruction üß¨",
    "section": "Polygons",
    "text": "Polygons\nThe second method of approximating images is using a population of polygons. Polygons are shapes with 3 or more points connected in no specific order. This could be a triangle, square but also also an hourglass shape has 4 points where the middle lines cross. While we could randomly add or remove points, in this case I chose to leave it at 3 points.\n\nIndividuals\nThe individuals here are slightly more complex than the pixel versions. That is because the polygons have their own class. Each individual‚Äôs genes are composed of a starting amount of 10-20 polygons.\n\n\nCrossover\nCrossovers are done simply by randomly taking a polygon of either parents. If one of the parents has more polygons, we randomly add polygons in those indices as well.\n\ndef crossover(self, other):\n    child = copy.deepcopy(self)\n    for idx, poly in enumerate(other.polygons):\n        if 0.5 &lt; np.random.random():\n            try:\n                child.polygons[idx] = poly\n            except:\n                # When idx is out of range of child.polygons, we append.\n                child.polygons.append(poly)\n    return child\n\n\n\nMutation\n\nIndividual\nMutation in this case has to be done at the ‚Äúgenes‚Äù level rather than the individual level. So the mutate method of the individuals here is surpisingly simple.\n\ndef mutate(self):\n    \"\"\"mutate method for an individual\"\"\"\n    for polygon in self.polygons:\n        polygon.mutate()\n\n    # deletion chance\n    if np.random.random() &lt; self.add_or_del_p and len(self.polygons) &gt; 1:\n        idx = np.random.randint(0, len(self.polygons) - 1)\n        self.polygons.pop(idx)\n\n    # addition chance\n    if np.random.random() &lt; self.add_or_del_p:\n        self.polygons.append(\n            Polygon(\n                canvas_size=self.canvas_size,\n                mutate_delta=self.mutate_delta,\n                mutate_p=self.mutate_p,\n            )\n        )\n\nThere are three steps:\n\nMutate each polygon individually (i.e.¬†change the location of the points and the colour)\nRandomly delete a polygon\nRandomly add a polygon\n\n\n\nGenes\n\n    def mutate(self):\n        \"\"\"mutate method for a gene\"\"\"\n        changed = False\n        for coord in self.coords:\n            if self.mutate_p &gt; np.random.random():\n                changed = True\n                coord[0] += np.random.randint(0, int(self.x_size * self.mutate_d)) - int(\n                    self.x_size * self.mutate_d * 0.5\n                )\n                coord[0] = np.clip(coord[0], 0, self.x_size)\n                coord[1] += np.random.randint(0, int(self.y_size * self.mutate_d)) - int(self.y_size * self.mutate_d * 0.5)\n                coord[1] = np.clip(coord[1], 0, self.y_size)\n\n        if self.mutate_p &gt; np.random.random():\n            self.color += (np.random.random(self.color_c) * self.mutate_d) - (self.mutate_d * 0.5)\n            self.color = np.clip(self.color, 0, 1)\n\n        if changed:\n            assert (0 &lt;= self.color).all() and (self.color &lt;= 1.0).all()\n            for x, y in self.coords:\n                assert 0 &lt;= x &lt;= self.x_size\n                assert 0 &lt;= y &lt;= self.y_size\n            self.calc_size()\n\nThis mutation is conceptually straight forward:\n\nMutate the location of each point with mutate_d as the fractional delta (i.e.¬†0.1 is 10% variation in position) based on the whole canvas width and height.\nMutate the colour of each polygon slightly. The approach is similar to the location, with mutate_d indicating a fraction delta. In this case the max is 1, so taking the random output is enough here. Because our colour is RGB, we input the self.color_c into random to get the desired number of channels (3).\n\nTo see what this looks like if we just have some polygons and keep mutating them.\n\n\n\n\n\n\nFigure¬†5: Mutations of several polygons over time\n\n\n\n\n\n\nPenalty\nBecause we do not want a model that just keeps adding polygons, we will penalize based on the number of polygons. The problem here is that you do not want to penalize it too much. If the difference of the penalty of one extra polygon is greater than the average difference in fitness between different individuals, then just removing a polygon will almost always be the best option. This happened at numerous attempts, and you just end up with an empty white grid very fast.\nThe penalty was calculated as follows: \\(p = 1 + \\lvert pop\\rvert * r_{pen}\\) where \\(p\\) is the eventual penalty, \\(\\lvert pop\\rvert\\) is the total population size and \\(r_{pen}\\) is the penalty rate. The penalty is applied as multiplier to the loss, which is the same as before: MSE. Hence the computation becomes \\(L_{pen} = L * p\\) where \\(L\\) and \\(L_{pen}\\) indicate the loss initially and penalized, respectively.\n\n\nPopulation crossover\nThe last interesting section is how to achieve the population crossover here.\n\n    def combine(self):\n        pairs = list(permutations(self.pop, r=2))\n        sample_pairs = random.choices(pairs, k=self.popsize)  # Sample popsize pairs out of all combinations. N out of N*(N-1)\n\n        # sample all permutations in the sample_top_n percent of the population.\n        sample_pairs += list(\n            permutations(self.pop[: int(len(self.pop) * self.sample_top_n)], r=2)\n        )\n\n        # Create copies of the copy_top_perc fraction of the population\n        children = [\n            self.get_best().copy() for _ in range(max(int(self.copy_top_perc * len(self.pop)), 2))\n        ]\n\n        # Add crossover of the picked pairs and add to the children.\n        children += [x.crossover(y) for (x, y) in sample_pairs]\n        self.pop += children\n\nAs stated before, we want to have a good balance between selecting fit individuals but also retaining the variance of the whole population. I try to find a balance in this by both selecting from the total population while also sampling the top x percent more than the rest and even copying the top y percent.\n\n\nResults\n\nPolygon Fox\nSince we are playing with polygons, I decided a target that was made of polygons might be suitable. This fox has some clear advantages for polygons, with big blue areas and a pointy bright fox head in the middle.\n\n\n\n\n\n\nFigure¬†6: Best individual using polygon approach, target and two loss metrics of a fox image\n\n\n\nWe observe a sharp incline in the number of polygons, even though it does not really seem to result in a clear improvement in fit. It seems the penalty was not tuned appropriate for this case. In this middle, we do see the fox head shape become apparant, but the algorithm fails to fill in the finer details of the head.\n\n\nRick & Morty\n\n\n\n\n\n\nFigure¬†7: Best individual using polygon approach, target and two loss metrics of a rick and morty image\n\n\n\nIt‚Äôs clear that this image has too much detail for this algorithms. Many of the planes are too small to easily get picked up by the mean-square-error reduction. The best fit is often achieved by having big polygons match up with the background. Additionally, the meandering of the number of polygons is a clear indication that the penalty was nicely balanced, since there was no clear preference.\n\n\nHyper parameter grid search\n\n\n                                                \n\n\nTo verify some results, I ran a hyper parameter search. I fixed the mutation probability and delta for this experiment, but in hindsight I should run it to include those.\nFrom the figure, if you select only the lowest losses, it becomes clear that most of the hyper parameters do not significantly impact the outcome. Going down, we first see that the addition/deletion probability starts to fade, with only the fewer values returning low losses. After that, the sample_top_n=0 is dropped, meaning that some sampling of the top candidates does improve results.\nIt is interesting to see that the number of polygons doesn‚Äôt seem to have a big impact, and it‚Äôs strange to see copy_top_perc not have much impact while the sample_top_n does. They should represent some level of similarity in their results.\n\n\n\nFuture improvements\nFor the pixel approach, a more balanced approach between crossover and mutation seems appropriate. It‚Äôs clear the algorithm quickly descents to a point where only mutations seems to be carrying improvements. Tuning the ratios to have a healthier balance with crossover would hopefully result in a slower but longer and more persistent decline of the loss.\nFor the polygon approach, I think one possible improvement is to have polygons that work on different scales. This means that we would have some polygons that work on large areas and other on small areas and we restrict the number in both or fix them to some proportion. That might allow the algorithm to fill in some finer details while also filling the big areas.\nAdditionally, the same improvement for the polygons might have to be made as for the pixels. The polygons also quickly seemed to descent into a situation where all individuals are very similar, hence more and stickier crossing over would be good.\nLastly, functionality that allows the addition and deletion of polygon points might bring an interesting variation. This could support pentagons, hexagons and more."
  },
  {
    "objectID": "posts/2023-02-15-genetic-algorithms.html#conclusion",
    "href": "posts/2023-02-15-genetic-algorithms.html#conclusion",
    "title": "Genetic Algorithms for image reconstruction üß¨",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post I showed two approaches that can replicate an image given some target using genetic algorithms. Both approaches have some advantages and some drawbacks, but both have their use. While this example is mainly for academic purposes, the core genetic algorithm has many applications and this practice both improves understanding and awareness of it.\nPersonally, I‚Äôve really enjoyed working on this project with both the academic understanding as well as the engineering of it being sometimes challenging. I hope you, the reader, has learned something from this post and can use it in your own work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Who am I?",
    "section": "",
    "text": "I‚Äôm a deep learning practitionerüß†/data scientistüìà. I have a background in computer science and data science, and thus both are very prevelant in my work. I have created some bots, some deep learning implementations and some libraries related to synthetic data.\n\nüî≠ I‚Äôm currently working on setting up a personal website and synthetic data using GANs.\nüå± I‚Äôm currently learning Full Stack (Deep/machine) Learning\nüí¨ Ask me about PyTorch and SOTA NLP or Computer Vision models! üî•üî•\nüòä My favorite machine learning sources are HuggingFace, Sotabench and paperswithcode"
  },
  {
    "objectID": "about.html#master-thesis",
    "href": "about.html#master-thesis",
    "title": "Who am I?",
    "section": "Master Thesis",
    "text": "Master Thesis\n\nIn my thesis, I researched improvements that we can make to Generative Adversarial Networks (GANs), to apply them better to tabular data. Contrary to GANs for vision tasks, GANs for tabular data are still very early work with only some researchers working on it. Apart from two improvements to the GAN architecture, I also wrote an open source library that focuses on how to evaluate synthetic data. You can find the github repos and the thesis PDF below.\n\n\n\n\nRepo for my thesis: Star\nRepo for evaluation library:  Star\nFind my thesis on the Radboud University website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Genetic Algorithms for image reconstruction üß¨\n\n\n\n\n\n\nGenetic Algorithms\n\n\nMachine learning\n\n\n\nHow we can leverage genetic algorithms to help with image reconstruction.\n\n\n\n\n\nFeb 16, 2023\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my Spotify listening history üéµ - Part 3\n\n\n\n\n\n\nAnalysis\n\n\nMusic\n\n\nBI\n\n\n\nIn part 3 of this adventure we discover the audio features that Spotify attributes to songs. We see how they are influenced by specific genres and how some songs relate to different audio features like instrumentalness and energy.\n\n\n\n\n\nAug 20, 2020\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my Spotify listening history üéµ - Part 2\n\n\n\n\n\n\nAnalysis\n\n\nMusic\n\n\nBI\n\n\n\nIn part 2 of this series, we investigate how my genres have developed over time. We find interesting pattern with regards to some holidays and a general trend towards hip hop. Additionally, we see what some good ways of visualization are for these insights.\n\n\n\n\n\nAug 7, 2020\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my Spotify listening history üéµ - Part 1\n\n\n\n\n\n\nAnalysis\n\n\nMusic\n\n\nBI\n\n\n\nSoul searching through my choices in music. Using my spotify listening data, we perform a dive into my listening behaviour and how it changed over time. We also make a start at analyzing the evolution of genres.\n\n\n\n\n\nJul 31, 2020\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated product recognition for hospitality industry insights üç∫\n\n\n\n\n\n\nNLP\n\n\ninternship\n\n\n\nBridging the gap to a data driven hospitality industry business intelligence on raw data.\n\n\n\n\n\nJul 24, 2020\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to personal website\n\n\n\n\n\n\nintroduction\n\n\n\nIntroduction to personal website.\n\n\n\n\n\nJul 23, 2020\n\n\nBauke Brenninkmeijer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "",
    "text": "In part 3 we are going to take a look at the audio features that Spotify labels songs with. They have 9 self-determined variables, that all indicate different aspects of a song. You‚Äôll learn all about them below. In part 2, we looked at how my genres are and changed over time. Now we‚Äôll take a look at how the audio features correspond to the changes in genres and how the genres are defined in the sense of these features.\nThe main research questions of part 3 are:\nLets start!"
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#danceability",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#danceability",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "Danceability",
    "text": "Danceability\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\nHere, we take a look at danceability. Given my main genres, we should see higher danceability than normally, with EDM being a big part of my listening. And indeed we do see this. In my distribution, we see quite some peaks around 0.8, while the general distribution has only a very small amount of songs with danceability higher than 0.8. Since we only have an image from spotify (and no actual data points), we cannot compare means, but I expect the general distribution mean to be somewhere between 0.5-0.6, while mine is 0.67"
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#energy",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#energy",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "Energy",
    "text": "Energy\n\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\nNow, energy is a bit strange. In my year in review, Spotify always telling me I listen to so much energetic music. However, looking at this plot, I seem to be way lower than the general distribution.\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs see if we get some more clarity if we look at the energy per artist.\n\n\n\n\n\n\nImportant\n\n\n\nFor this table and all that follow, the count is just the frequency count of that value, whatever that value might be. If it is an artist, it is the total number of times that artist was played. If it is a song, it is the number of times that song was played.\n\n\n\n\n\n\n\n\n\n\n\nenergy\n\n\n\ncount\nmean\n\n\nmaster_metadata_album_artist_name\n\n\n\n\n\n\n6ix9ine\n260\n0.699400\n\n\nKIDS SEE GHOSTS\n302\n0.691255\n\n\nFresku\n264\n0.657288\n\n\nTchami\n370\n0.644843\n\n\nNetsky\n466\n0.639095\n\n\nKid Cudi\n853\n0.634662\n\n\nKendrick Lamar\n2740\n0.633111\n\n\nMac Miller\n527\n0.628738\n\n\nFlume\n593\n0.627781\n\n\nTravis Scott\n265\n0.623781\n\n\nYellow Claw\n2533\n0.618704\n\n\nVarious Artists\n1594\n0.618487\n\n\nEminem\n1567\n0.616364\n\n\nYung Internet\n373\n0.607120\n\n\nG-Eazy\n926\n0.605303\n\n\nLil Dicky\n303\n0.588901\n\n\nKanye West\n1070\n0.571160\n\n\nParov Stelar\n264\n0.563025\n\n\nVitalic\n275\n0.543753\n\n\nFrank Sinatra\n804\n0.512106\n\n\n\n\n\n\n\nThese results I find quite surprising. Yellow Claw, a Dutch trap duo whom I would consider to be generally pretty high energy is equal or below quite some hip hop artists, like Mac Miller and Kid Cudi. We can see the maximum energy in here is 6ix9ine, with a solid 0.69 üòÇ. There is no denying that 6ix9ine is at that level of energy. However, KIDS SEE GHOSTS, the front for Kid Cudi and Kanye West, somehow also has 0.69, while I‚Äôd say that is much lower in energy. If we look at Reborn, the most popular song on the album, it‚Äôs clearly a pretty slow song.\n\nAlternatively, we can look at the energy of my top songs. Here we get a clearer picture of the effect of different songs.\n\n\n\n\n\n\n\n\n\n\nenergy\n\n\n\n\ncount\nmean\n\n\nmaster_metadata_album_artist_name\nmaster_metadata_track_name\n\n\n\n\n\n\nKendrick Lamar\nHUMBLE.\n125\n0.6210\n\n\nDNA.\n124\n0.5230\n\n\nELEMENT.\n93\n0.7050\n\n\nLOYALTY. FEAT. RIHANNA.\n81\n0.5350\n\n\nYAH.\n81\n0.7000\n\n\nFEEL.\n80\n0.7950\n\n\nTravis Scott\ngoosebumps\n99\n0.7280\n\n\nYellow Claw\nCity on Lockdown (feat. Juicy J & Lil Debbie)\n96\n0.8410\n\n\nGood Day (feat. DJ Snake & Elliphant)\n87\n0.5660\n\n\nOpen (feat. Moksi & Jonna Fraser)\n87\n0.5920\n\n\nWithout You (feat. The Galaxy & Gia Koka)\n82\n0.4880\n\n\nInvitation (feat. Yade Lauren)\n80\n0.6620\n\n\nLove & War (feat. Yade Lauren)\n78\n0.5330\n\n\nLast Paradise (feat. Sody)\n78\n0.5930\n\n\nStacks (feat. Quavo, Tinie Tempah & Cesqeaux)\n77\n0.0812\n\n\n\n\n\n\n\nThere are some pretty interesting facts here. First of, we can see see that my idea of Yellow Claw being very high energy is debunked, at least with Spotify‚Äôs definition of energy. Because some of these values are very strange, even more so when compared to some other artists and songs. Take for example the song without you from Yellow Claw. This is a dubstep/brostep song, with a pretty high tempo but has a value of 0.4880 for its energy level. This is lower than Frank Sinatra had on average in the previous table. In my opinion, something is wrong there. Furthermore, we can also see that their song Stacks has an energy value of 0.0812, which, by all accounts, should be an anomaly. My expectation is that they trained a neural network with some hand labeled songs, and then applied that to all songs to estimate these values, and something went wrong in the case of this song."
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#instrumentalness",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#instrumentalness",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "Instrumentalness",
    "text": "Instrumentalness\n\nPredicts whether a track contains no vocals. ‚ÄúOoh‚Äù and ‚Äúaah‚Äù sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly ‚Äúvocal‚Äù. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\n\n\n\n\n\n\n\n\n\nThis is potentially interesting, since, naturally, hip-hop has a lot of vocals (so low instrumentalness), but genres like electro house and EDM generally don‚Äôt have many vocals, so how is this plot explained with almost nothing around 1. We can quickly see why by looking at the most influential artists for the largest genres. I‚Äôm showing the top 10 most played artists with their largest genre (e.g.¬†if an artist is tagged as ‚Äòrap‚Äô and ‚Äòwest coast hip hop‚Äô, we use the more general rap)\n\n\n\n\n\n\n\n\n\n\ncount\n\n\ntop_1_genres\nmaster_metadata_album_artist_name\n\n\n\n\n\nedm\nYellow Claw\n2531\n\n\nFlume\n592\n\n\nTchami\n370\n\n\nrap\nKendrick Lamar\n2738\n\n\nEminem\n1564\n\n\nKanye West\n1065\n\n\nG-Eazy\n926\n\n\nKid Cudi\n852\n\n\nMac Miller\n527\n\n\nLil Dicky\n303\n\n\n\n\n\n\n\nAs most influential EDM artists, we see Yellow Claw and Flume, both of which have a lot of vocals in their music. This explains our instrumentalness plot!.\nHowever, if we redo this analysis, but we drop the most general genres, we get much more interesting results. To be specific, I‚Äôve ignored the values pop, edm, rap, pop rap and hip hop, unless there were no other genre labels. In that case, we still take that genre. The result is below, and quite interesting. The table shows the top 20 most played artists and the genre they belong to.\n\n\n\n\n\n\n\n\n\n\nmaster_metadata_album_artist_name\n\n\ntop_1_genres_filtered\nmaster_metadata_album_artist_name\n\n\n\n\n\nchicago rap\nKanye West\n1065\n\n\nchristmas\nFrank Sinatra\n804\n\n\nconscious hip hop\nKendrick Lamar\n2738\n\n\ndetroit hip hop\nEminem\n1564\n\n\ndowntempo\nFlume\n592\n\n\nParov Stelar\n263\n\n\ndutch hip hop\nYung Internet\n373\n\n\nFresku\n263\n\n\nelectro house\nYellow Claw\n2531\n\n\nTchami\n370\n\n\nVitalic\n274\n\n\nemo rap\n6ix9ine\n260\n\n\nhouse\nFISHER\n247\n\n\nindie pop rap\nG-Eazy\n926\n\n\nliquid funk\nNetsky\n466\n\n\nrap\nKid Cudi\n852\n\n\nMac Miller\n527\n\n\nLil Dicky\n303\n\n\nKIDS SEE GHOSTS\n302\n\n\nTravis Scott\n264\n\n\n\n\n\n\n\nWithout the super general genres, we can finally start to see some trends. Given the fact that rap is still the main genre of 5 artists, this often seems to be one of few labels applied, possibly with the other labels I was removing. The results is that these artists do not belong to any specific subgenres within hip hop, which is cool to see. Does this also mean they are per definition mainstream?\n\n\n\n\n\n\nNote\n\n\n\nIn part 2, the question arose of why emo rap was one of my main genres in some months. This is also explained by this plot, because 6ix9ine is regarded as emo rap by spotify, and his presence in my listening is very varied, with sometimes none and sometimes quite a bit."
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#loudness",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#loudness",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "loudness",
    "text": "loudness\n\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\n\n\n\n\n\n\n\n\n\nWe can see my music is quite a bit louder than average, with almost all of the mass of the distribution being between -10 and 0, with a small tail between -15 and -10. In the general distribution, the tail extends quite a bit beyond -20, whereas there is virtually nothing beyond -18 in my distribution. Let‚Äôs see who is responsible for all of this noise üò†. Let‚Äôs plot the loudness of my top 20 most listened artists.\n\n\n\n\n\n\n\n\n\nloudness\n\n\n\ncount\nmean\n\n\nmaster_metadata_album_artist_name\n\n\n\n\n\n\n6ix9ine\n260\n-5.624454\n\n\nTravis Scott\n265\n-5.832879\n\n\nKid Cudi\n853\n-6.496838\n\n\nKanye West\n1070\n-6.535216\n\n\nKIDS SEE GHOSTS\n302\n-6.612129\n\n\nFresku\n264\n-6.614091\n\n\nNetsky\n466\n-6.690002\n\n\nG-Eazy\n926\n-6.849693\n\n\nYellow Claw\n2533\n-6.875627\n\n\nKendrick Lamar\n2740\n-7.033027\n\n\nEminem\n1567\n-7.094228\n\n\nVarious Artists\n1594\n-7.189992\n\n\nTchami\n370\n-7.304330\n\n\nFlume\n593\n-7.348245\n\n\nMac Miller\n527\n-7.576696\n\n\nParov Stelar\n264\n-7.825705\n\n\nYung Internet\n373\n-7.844906\n\n\nVitalic\n275\n-8.272200\n\n\nFrank Sinatra\n804\n-8.528413\n\n\nLil Dicky\n303\n-9.120274\n\n\n\n\n\n\n\nThis table is pretty clear. We see that none of my top 20 most played artists have an average loudness lower than -10, indicating quite high loudness on average. If we look at the top 5, we see that apparently hip hop artists are very loud, but also specifically Kid Cudi, Kanye West, and then those two combined in KIDS SEE GHOSTS. The reason for this I expect to be that they have sounds playing at almost at all times, rather than that they are extremely loud in their peaks.\nIt also comes as no surprise that 6ix9ine is the loudest, since he‚Äôs essentially screaming in most of his songs üòÖ."
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#valence",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#valence",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "valence",
    "text": "valence\n\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.¬†happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.¬†sad, depressed, angry).\n\nThis is a very interesting feature. Let‚Äôs see what information it gives us!\n\n\n\n\n\n\n\n\n\n\nThat‚Äôs quite the difference! First off, we can see a clear decline towards 1.0 for my distribution, whereas in the general distribution, this is much rounder. Furthermore, we can see that my distribution a lot spikier, possibly indicating the effects of certain albums/songs that have been played a lot and have a very narrow spread with regards to valence. In general we can see my music taste leans more towards the low side of valence, indicating a preference for sad, depressed and angry music. In general, I think it will be angry, which is the sentiment in a lot of hip hop, but also techno, drum and bass and some parts of house.\nIf we look at the most positive and most negative songs with more than 5 plays, we see some interesting results.\n\n\n\n\n\n\n\n\n\n\nvalence\n\n\n\n\ncount\nmean\n\n\nmaster_metadata_album_artist_name\nmaster_metadata_track_name\n\n\n\n\n\n\nLil Dicky\nWhite Crime\n19\n0.00000\n\n\nEminem\nWhite America\n31\n0.00000\n\n\nKanye West\nFade\n37\n0.00000\n\n\nEllen Allien\nStormy Memories\n10\n0.00001\n\n\nHollen\nSleeping Dogs - Original\n11\n0.01400\n\n\n...\n...\n...\n...\n\n\nEd Sheeran\nShape of You\n9\n0.93100\n\n\nFresku\nChickie (skit)\n15\n0.93800\n\n\nVictor Ruiz\nBrujeria\n7\n0.96000\n\n\nKendrick Lamar\nMomma\n36\n0.96300\n\n\nYung Internet\nHelemaal Top (feat. Donnie)\n48\n0.96300\n\n\n\n\n1171 rows √ó 2 columns\n\n\n\nWe can see that there are likely some errors in here, with White Crime and White America both having a score of 0.0. On the positive side, we see Yung Internet with Helemaal Top, which is a song about feeling great. Furthermore, we have Kendrick Lamar‚Äôs Momma. This is interesting cause it does not typically resonate as a very happy or cheerful song. We‚Äôll attribute this to the Spotify interpretation of this value. See for yourself:\n\nIf we take a higher level view, we can look at the artists and identify more broader trends.\n\n\n\n\n\n\n\n\n\nvalence\n\n\n\ncount\nmean\n\n\nmaster_metadata_album_artist_name\n\n\n\n\n\n\nKanye West\n1070\n0.327687\n\n\nTchami\n370\n0.360095\n\n\nNetsky\n466\n0.383689\n\n\nKIDS SEE GHOSTS\n302\n0.414606\n\n\nYellow Claw\n2533\n0.430344\n\n\nFrank Sinatra\n804\n0.434713\n\n\nLil Dicky\n303\n0.445153\n\n\nG-Eazy\n926\n0.445842\n\n\nMac Miller\n527\n0.462410\n\n\nKendrick Lamar\n2740\n0.469246\n\n\nVarious Artists\n1594\n0.476991\n\n\nKid Cudi\n853\n0.486193\n\n\nEminem\n1567\n0.493737\n\n\nYung Internet\n373\n0.511973\n\n\nFlume\n593\n0.575045\n\n\n\n\n\n\n\nSome interesting insights: 1. Kanye is depressing: Kanye West is very low in valence, meaning most of his songs are angry, depressed or sad. This makes sense, given his oeuvre, with songs like Waves (sad), Violent Crimes (sad), Piss On Your Grave (angry) and I Am A God (angry). 2. Bias of electronic music: We have some electronic artists like Tchami and Netsky which also rank very low, but which are not specifically angry or sad music producers in my experience. Maybe electronic music has a bias here and is faster to be considered angry or sad? 3. Non-polarity: We see quite some artists hovering around 0.5, indicating either a healthy balance in valence between their songs or just a general non-polarity in their songs. I took a detailed look at Kanye West for the first point, and he has a wide spread, with the weight more on the sad and angry side, hence his low average valence. I assume most artists will be similar, and have a wide spread between their songs."
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#energy-in-rap-and-edm",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#energy-in-rap-and-edm",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "Energy in Rap and EDM",
    "text": "Energy in Rap and EDM\n\n\n\n\n\n\nImportant\n\n\n\nFrom here one out, all plots are again interactive. Try to disable some things in the legend!\n\n\nGiven our results in previous sections, one of the things I‚Äôd like to look deeper into is the effects of EDM and rap with regards to energy.\n\n\n\n\n\n\n\nAbove, we see the Kernel Density Estimation (KDE) of energy for rap and EDM. Rap is strange, since it seems to have two distinct peaks; one around 0.6 and one around 0.75. EDM has a more well defined single peak at 0.6. Furthermore, we see that EDM is more prevalent in the high energy values, which is not too unsurprising to see. Let‚Äôs dive a bit deeper into the two peaks of rap.\nIf we plot different hip hop subgenres, we can see how these peaks come to be. We plot the largest three hip hop subgenres. Detroit hip hop seems to have more of a peak around 0.75, while conscious hip hop and chicago rap have peaks around at 0.6. It seems that they have a lot more energy in Detroit!"
  },
  {
    "objectID": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#valence-in-rap-and-edm",
    "href": "posts/2020-08-20-spotify-listening-history-analysis-part-3.html#valence-in-rap-and-edm",
    "title": "Analyzing my Spotify listening history üéµ - Part 3",
    "section": "Valence in Rap and EDM",
    "text": "Valence in Rap and EDM\n\n\n\n\n\n\n\nWe can see some interesting differences in valence for these genres. We observe that EDM is more prevalent in the low valence values (0.0-0.2), dropping to zero much later than rap. Rap, on the other hand, is more prevalent in the 0.3-0.5 range than EDM. Now, we still don‚Äôt know if 0.5 valence means it is neutral, but it can tell us at least that rap is on average less sad and angry than EDM, which I find an interesting discovery, since I would have guessed it the other way around."
  },
  {
    "objectID": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html",
    "href": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html",
    "title": "Analyzing my Spotify listening history üéµ - Part 2",
    "section": "",
    "text": "In part 1 of this series we looked at the first part of this project. This included: 1. The data we are working with and what it looks like. 2. The amount of listening done per year and per month. 3. The amount of listening done per hour of day, also throughout the years. 4. The amount of genres we have per song/artist.\nWe will continue from where we left of, diving deeper into the genres.\nWe‚Äôll load up the original JSON from Spotify, as well as the genres we created in part 1. We then combine them into comb, the combined dataframe. In genres.csv, we again see the 20 columns with the genres for each song, where the genres are collected from the artist, since songs are not labeled as having a genre. For more details, please have a look at part 1.\n\n# data received from Spotify\ndf.head(1)\n\n\n\n\n\n\n\n\nts\nms_played\nconn_country\nmaster_metadata_track_name\nmaster_metadata_album_artist_name\nmaster_metadata_album_album_name\nreason_start\nreason_end\nshuffle\nskipped\n...\ncity\nregion\nepisode_name\nepisode_show_name\ndate\nyear\nmonth\nday\ndow\nhour\n\n\n\n\n0\n2013-10-09 20:24:30+00:00\n15010\nNL\nWild for the Night (feat. Skrillex & Birdy Nam...\nA$AP Rocky\nLONG.LIVE.A$AP (Deluxe Version)\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\n2013-10-09\n2013\n10\n9\n2\n20\n\n\n\n\n1 rows √ó 25 columns\n\n\n\nGenres retrieved from Spotify and the combined dataframe. We rename the genres columns from just a number 0-20 to ‚Äògenre_x‚Äô with x between 0 and 20, so they‚Äôre easier to recognize.\ncomb consists of df + genres_df, with the genre columns at the end.\n\n# genres retrieved through Spotify API\ngenres_df = pd.read_csv('genres.csv', low_memory=False)\ngenres_df = genres_df.rename(columns={str(x): f'genre_{x}' for x in range(21)})\ncomb = pd.concat([df, genres_df], axis=1)\ncomb.head(2)\n\n\n\n\n\n\n\n\nts\nms_played\nconn_country\nmaster_metadata_track_name\nmaster_metadata_album_artist_name\nmaster_metadata_album_album_name\nreason_start\nreason_end\nshuffle\nskipped\n...\ngenre_11\ngenre_12\ngenre_13\ngenre_14\ngenre_15\ngenre_16\ngenre_17\ngenre_18\ngenre_19\ngenre_20\n\n\n\n\n0\n2013-10-09 20:24:30+00:00\n15010\nNL\nWild for the Night (feat. Skrillex & Birdy Nam...\nA$AP Rocky\nLONG.LIVE.A$AP (Deluxe Version)\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2013-10-09 20:19:20+00:00\n68139\nNL\nBuzzin'\nOVERWERK\nThe Nth¬∫\nunknown\nclick-row\nFalse\nFalse\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2 rows √ó 46 columns"
  },
  {
    "objectID": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html#top-genres-with-percentages",
    "href": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html#top-genres-with-percentages",
    "title": "Analyzing my Spotify listening history üéµ - Part 2",
    "section": "Top genres with percentages üìä",
    "text": "Top genres with percentages üìä\n\n\nCode\nselection = alt.selection_multi(fields=['genre'], bind='legend')\n\nnormalized = alt.Chart(top_genres_per_month_with_perc).mark_bar().encode(\n    x=alt.X('yearmonth(datetime):O', title='Month per year'),\n    y=alt.Y('percentage', stack='normalize', title='Normalized percentage (%)'),\n    color = alt.Color(\n        'genre',\n        scale=alt.Scale(\n           scheme='tableau20',\n        )\n    ),\n    tooltip=['genre', 'percentage', 'yearmonth(datetime)'],\n    order=alt.Order(\n      'percentage',\n      sort='descending'\n    ),\n    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n).properties(\n    title='Normalized percentage occurences of top 5 genres per month',\n    width=MAXWIDTH\n).add_selection(\n    selection\n)\n\nnon_normalized = alt.Chart(top_genres_per_month_with_perc).mark_bar().encode(\n    x=alt.X('yearmonth(datetime):O', title='Month per year'),\n    y=alt.Y('percentage', title='Percentage (%)'),\n    color = alt.Color(\n        'genre',\n        scale=alt.Scale(\n           scheme='tableau20',\n        )\n    ),\n    tooltip=['genre', 'percentage', 'yearmonth(datetime)'],\n    order=alt.Order(\n      'percentage',\n      sort='descending'\n    ),\n    opacity=alt.condition(selection, alt.value(1), alt.value(0.2))\n).properties(\n    title='Percentage occurences of top 5 genres per month', \n    width=MAXWIDTH\n).add_selection(\n    selection\n)\n\nnon_normalized & normalized\n\n\n\n\n\n\n\nThere are definitely some interesting things in theses plots. We can see some consistent attendees that we also saw in the most listened genres in general, so that‚Äôs not a big surprise. For example, these include rap, edm and hip hop.\n\nSeasonal effects: What is quite interesting is to see when the very common genres are not dominating the chart, like in December of 2016. Both in November and December of 2016 we see I was in a very strong Christmas mood, with christmas covering 16% of songs in November and 51%(!) in December. The top genres in December are adult standard, easy listening, christmas and lounge. Those definitely are in the same segment, with my listening, so it‚Äôs not surprising that those other genres appear alongside Christmas in a heavy Christmas month. This is because my Christmas music is more focused on the 40s and 50s, with artists like Frank Sinatra and Dean Martin, rather than Mariah Carey. We do not see this seasonal effect in 2017 and 2018, but those years my Christmas music urge was just less, so this drop is explainable. Instead of Christmas, in December of 2018 emo rap is in my top 5 genres ü§î. That might be interesting to look at in another blog post.\nElectronic periods: Something else that stands out is that there are electronic music periods, like June, July and August of 2017 and January of 2018. However, both edm and electro house are present in essentially each month as high scorers, so I‚Äôm definitely a fan in general. But these peak months still stand out.\nRise of Rap: The last thing that is interesting is probably the fact that rap and hip hop have almost exclusively been the top 2 from February 2018 to January 2019. This indicates a move away from the more electronic genres and more towards hip hop. A possible reason for this might be the move towards more set-based plays for electronic music, which are generally not on Spotify, but on platforms like Youtube. Otherwise, it might just be an actual preference shift. However, I do still listen to a lot of these types of music, so I suspect the former. Looking at data from 2019 and 2020 might give some insight in this."
  },
  {
    "objectID": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html#top-genres-without-percentages",
    "href": "posts/2020-08-07-spotify-listening-history-analysis-part-2.html#top-genres-without-percentages",
    "title": "Analyzing my Spotify listening history üéµ - Part 2",
    "section": "Top genres without percentages üèÜ",
    "text": "Top genres without percentages üèÜ\nSo we‚Äôve seen how the genres relate to each other in terms of percentages per month. We can also see what the top genres are per month, but it can definitely still be improved. I really just want a list with the top 5 genres per month, ideally easily readable and pretty close to the example we had from Last.fm.\nAs a reminder, that looked like this:\n\n\n\nYour top genres, plotted per week.\n\n\nWe can get a list of the top genres per month by grouping and then applying list on the Series.\n\ntop_genres_per_month = top_genres_per_month_with_perc.groupby(['year',  'month']).genre.apply(list).reset_index()\ntop_genres_per_month[:2]\n\n\n\n\n\n\n\n\nyear\nmonth\ngenre\n\n\n\n\n0\n2016\n9\n[rap, pop rap, hip hop, pop, indie pop rap]\n\n\n1\n2016\n10\n[edm, pop, rap, electro house, hip hop]\n\n\n\n\n\n\n\nWe then create a numpy array from these values and apply them column by column to new dataframe columns.\n\ngenre_array = np.stack(top_genres_per_month.genre.values)\nfor i, new_col in enumerate([f'genre_{x}' for x in range(1, 6)]):\n    top_genres_per_month[new_col] = genre_array[:, i]\ntop_genres_per_month = top_genres_per_month.drop('genre', axis=1)\n\nUntil we finally arrive at the following dataframe. On the x-axis we have the top 5 genres, named genre_1 till genre_5, while on the y-axis we have months per year. This is pretty much what I set out to do, so I‚Äôm happy with the result.\n\ntop_genres_per_month = top_genres_per_month.set_index(['year', 'month']).T\ntop_genres_per_month\n\n\n\n\n\n\n\nyear\n2016\n2017\n...\n2018\n2019\n\n\nmonth\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n...\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n\n\n\n\ngenre_1\nrap\nedm\nedm\nadult standards\npop\nelectro house\npop rap\nrap\nrap\npop\n...\nrap\nedm\nrap\nrap\nrap\nrap\nrap\nrap\nrap\nrap\n\n\ngenre_2\npop rap\npop\npop\neasy listening\nedm\nfilter house\nrap\npop rap\npop rap\nedm\n...\nhip hop\nrap\npop rap\nhip hop\nedm\nhip hop\nhip hop\nhip hop\nhip hop\nhip hop\n\n\ngenre_3\nhip hop\nrap\nadult standards\nchristmas\nrock\ndance-punk\nedm\nhip hop\nhip hop\nelectro house\n...\nedm\nelectro house\nhip hop\npop rap\nhip hop\npop rap\nedm\npop rap\npop rap\npop rap\n\n\ngenre_4\npop\nelectro house\nchristmas\nlounge\ndance pop\nelectronic\nhip hop\nconscious hip hop\npop\nbrostep\n...\npop\nhip hop\nedm\nedm\npop rap\nedm\npop rap\npop\npop\nedm\n\n\ngenre_5\nindie pop rap\nhip hop\neasy listening\ndutch hip hop\ntropical house\nalternative dance\npop\nwest coast rap\nconscious hip hop\nelectronic trap\n...\npop rap\npop\npop\nelectro house\nelectro house\npop\npop\nedm\nemo rap\nelectro house\n\n\n\n\n5 rows √ó 29 columns\n\n\n\nHowever, the lack of color makes interpreting this table still fairly challenging. Let‚Äôs see if we can improve that a bit.\nTo style, we can use the style (docs) attribute of pd.DataFrame. This is an easy and super handy way of styling dataframes. It has two main methods: .applymap and .apply. The first one is applied to each cell individually, while the latter is applied to a whole row. That makes .applymap well suited for cell specific layouts, like min-max gradients for example, while .apply works very well for row-based operations, like highlighting the max.\nTo use them, we need to define a coloring function to apply to the dataframe. As a parameter, we give all the unique values. This allows us to create a mapping, as well as define the number of colors required. The colors we use are RGB colors that aren‚Äôt from the standard coloring libraries, like seaborn color palette. This is because none of their palettes support the number of unique values we have, which is 26. So I used the tool called i want hue, that allows the generation of suitable color palettes. Getting 26 unique colors was still not easy (or a great succes in my opinion), but it works at least semi well.\n\n\nCode\nimport seaborn as sns\n\ncolors_26 = [\n    \"#85cec7\",\n    \"#f398d9\",\n    \"#afe084\",\n    \"#90a9f4\",\n    \"#c0c15c\",\n    \"#74aff3\",\n    \"#e4e88b\",\n    \"#d8afec\",\n    \"#64ddab\",\n    \"#f3a281\",\n    \"#52ebd9\",\n    \"#ebabbe\",\n    \"#9de5a0\",\n    \"#a2b8f0\",\n    \"#e6bb6d\",\n    \"#77cdef\",\n    \"#b8c270\",\n    \"#b6bee4\",\n    \"#9ac68a\",\n    \"#4cd1da\",\n    \"#dfc299\",\n    \"#a0ebe5\",\n    \"#c0c38e\",\n    \"#8cbca8\",\n    \"#d8ebb4\",\n    \"#a7e1c1\"\n]\n\ndef color_cells(val, unique_values):\n    \"\"\"\n    Takes a cell value and applies coloring depending on the value. Should be applied to a cell, not a row. So use `.applymap`. If value is unknown, defaults to white. \n    \"\"\"\n    # Multiply with 255 to get into css RGB range (0, 255) instead of (0, 1).\n    colors_arr = [tuple(int(y*255) for y in x) for x in sns.color_palette(colors_26)]  \n    colormap = [f'rgb{x}' for x in colors_arr]\n    colors = {k: v for k, v in zip(unique_values, colormap)}\n    color = colors.get(val, 'white')\n    return f'background-color: {color}'\n\n\n\n\nCode\nunique_top_genres = np.unique(top_genres_per_month)  # Get a list of unique values for coloring\ntop_genres_per_month.style.applymap(color_cells, unique_values=unique_top_genres)\n\n\n\n\n\n\nyear\n2016\n2017\n2018\n2019\n\n\nmonth\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n\n\n\n\ngenre_1\nrap\nedm\nedm\nadult standards\npop\nelectro house\npop rap\nrap\nrap\npop\nedm\npop\nrap\npop\nedm\nrap\nhouse\nrap\nrap\nrap\nedm\nrap\nrap\nrap\nrap\nrap\nrap\nrap\nrap\n\n\ngenre_2\npop rap\npop\npop\neasy listening\nedm\nfilter house\nrap\npop rap\npop rap\nedm\nelectro house\nedm\npop rap\nrap\nelectro house\npop\nedm\nhip hop\nhip hop\nhip hop\nrap\npop rap\nhip hop\nedm\nhip hop\nhip hop\nhip hop\nhip hop\nhip hop\n\n\ngenre_3\nhip hop\nrap\nadult standards\nchristmas\nrock\ndance-punk\nedm\nhip hop\nhip hop\nelectro house\npop\nelectro house\nhip hop\nedm\nrap\npop rap\nelectro house\npop\npop rap\nedm\nelectro house\nhip hop\npop rap\nhip hop\npop rap\nedm\npop rap\npop rap\npop rap\n\n\ngenre_4\npop\nelectro house\nchristmas\nlounge\ndance pop\nelectronic\nhip hop\nconscious hip hop\npop\nbrostep\nbrostep\nbrostep\npop\npop rap\npop rap\nhip hop\ntech house\npop rap\nedm\npop\nhip hop\nedm\nedm\npop rap\nedm\npop rap\npop\npop\nedm\n\n\ngenre_5\nindie pop rap\nhip hop\neasy listening\ndutch hip hop\ntropical house\nalternative dance\npop\nwest coast rap\nconscious hip hop\nelectronic trap\nelectronic trap\nelectronic trap\nedm\nelectro house\npop\nedm\npop\nedm\npop\npop rap\npop\npop\nelectro house\nelectro house\npop\npop\nedm\nemo rap\nelectro house\n\n\n\n\n\nBetter get the üöí cause this table is üî•.\nThis is really close to the Last.fm plot, apart from the lines between points that require 10 years of D3.js experience. We see some similar pattern to those in the earlier plot, but also can see some new insights. Here, we can focus some more on the anomalies that are present, like indie pop rap, dutch hip hop, filter house and conscious hip hop. These stand out more using this representation than before, which focused more on trends.\nInsights - More electronic peaks: We can see that February 2017 was actually also a peak in electronic music, but due to similar colors in the previous plot this was a bit hidden. - Pure hip hop periods: Furthermore, we can also see there are some pure hip hop periods, like April and May of 2017, where EDM and electro house are not present at all, and we see more specific hip hop genres make way like west coast rap and conscious hip hop."
  },
  {
    "objectID": "posts/2020-07-23-first-personal-post.html",
    "href": "posts/2020-07-23-first-personal-post.html",
    "title": "Introduction to personal website",
    "section": "",
    "text": "Introduction to personal website\nWelcome!\n\nAfter fighting with setting up a custom domain, I‚Äôve not officially set out to write about some interesting topics here. The main challenge for me, by length, will be to work towards rounding something of within a limited amount of time. I always get sucked into diving way too deep into a topic, to a point where I‚Äôm not knowledgeable enough anymore to understand, and then I lose interest :sweat_smile:. This time will definitely be different!\nWhat do I want to do here? I‚Äôd like to write about interesting stuff I find online, discover or think of! Yes, very original :). What does this mean, concretely?\nI‚Äôm going to talk about: - Deep learnig - Machine learning - Reinforcement Learning - Natural language processing - Computer Vision - Whatever else I find interesting. I can do what I want, mom!\nCurrent impediments for realizing my dreams: - How do you use latex here? \\(\\lambda\\), does this work? - Finding some good topics.\n\nThanks and hope to see you!"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Introduction to SQLAlchemy\n\n18-05-2023\nRepo | Slides\nDuring this talk, I discuss ORMs in python and consequently go into SQLAlchemy as a framework. I compare it against the main alternatives for SQL Server, which is PyODBC. I show the advantages of both as well as potential pitfalls and drawbacks. The conclusion of the talk is: in most more complex or larger projects, SQLAlchemy should have a clear preference over alternatives in Python.\n\n\n\nDiving into NLP Tokenizers - DSFC 2022\n\nApril 2022\nRepo | Slides 1 | Slides 2\nA deep-dive into NLP tokenizers - their differences, similarities and advantages.\nThis presentation was prepared for my talk at DSFC 2022. DSFC is a data science conference hosted by the largest 5 banks in the Netherlands.\nThe total presentation consists of two sets of slides. One non-interactive and one interactive. The non-interactive one you can see the easiest by clicking preview in github. The interactive slides are made in jupyter notebook with RISE, which converts a notebook into slides. You can view the notebook regardless, but if you want them as slides you will need the RISE plugin.\n\n\n\nPractical Data Science - tips, tricks and pitfalls\n\nApril 2022\nRepo | Slides\nThis talk, given at an ABN AMRO hackathon to prepare for a kaggle competition, was aimed at giving some practical tips to data scientists. The tips are aimed at people fairly new to the field but also those medior and hopefully even some seniors can learn something from it. The goal of these tips is to clarify some unexpected behaviour when working with data and models, as well as put a spotlight on some common pitfalls.\nIt covers six main topics:\n\nBusiness tips\nShort tip on models\nOrdinal/Nominal data encodings\nFeature Importance with Trees\nClass Imbalance\nOrder of pre-processing\n\n\n\nSome highlights:\n\nEffects of imbalanced data sampling techniques with lower amounts of data\n\n\n\neffects of using class weights visualized\n\n\n\n\nBehaviour of KNN with different encodings\n\n\n\nVisualization of the performance of the KNN algorithm given different encodings plotted over number of neighbours.\n\n\n\n\n\n\nCode first introduction to Machine Learning\n\nJuly 2020\nRepo | Slides\nHaven been an avid participant in Fast.ai, for most things I‚Äôm learning or teaching I try to take a code-first approach. In this talk, I presented some of the basics of machine learning to python developers.\nThe talk covers machine-learning basics such as:\n\nTraining a simple model\nInfluence of hyper parameters on perfromance.\nUnder- and overfitting\nPlotting decision trees\nTypes of machine learning\nFeature Engineering\n\n\n\n\nFakeFynder: Deepfake detection for the masses\n\nAugust 2019\nIn our submission for the Hackathon for Good, we created a working POC which is a website where people can easily paste youtube links or upload videos to have them be checked for manipulated sections. The deepfake detection is done using the model from FaceForensics++, which has around 80% accuracy on a combination of compressed videos, but achieves around 99% accuracy on a single type of compression.\nThe POC also allows for easy checking whether a video has been seen before with a database of video hashes which can be searched.\n\n\n\nUsage of the FakeFynder App\n\n\n\n\n\nOn the Generation and Evaluation of Synthetic Tabular Data using GANs - Master Thesis\n\nSeptember 2019\nRepo | Slides\nIn my thesis, I researched improvements that we can make to Generative Adversarial Networks (GANs), to apply them better to tabular data. Contrary to GANs for vision tasks, GANs for tabular data were/are still very early work with only some researchers advancing the field. Apart from two improvements to the GAN architecture, I also wrote an open source library that focuses on how to evaluate synthetic data. The presentation also goes into uses-cases and value created for companies, specifically ABN AMRO. You can find the github repos which includes the thesis PDF at the link above.\n\n\nSome highlight\n\nAlmost art, but not quite.These matrices show correlation between columns and the delta with the actual correlations.\n\n\n\nAn easy way to see if two distributions are actually the same - a plot of the cumsums of numeric columns."
  }
]